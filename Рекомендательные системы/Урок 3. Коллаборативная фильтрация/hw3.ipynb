{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Вспомним прошлый вебинар, мы рассматривали User-User рекомендации и Item-Item рекомендации. Чем они отличаются и чем они похожи? Если есть функция item_item_rec(interaction_matrix). Можно ли использовать эту функцию для user_user_rec?  \n",
    "В чем принципиальные отличия item-item рекомендаций от ALS?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе User-User определяется сходство между пользователями и в качестве рекомендаций пользователю выдается n самых часто покупаемых товаров k наиболее похожими на него покупателями. Для оценки степени схожести пользователей в плане их предпочтений могут использоваться различные функции сходства (метрики). Наиболее популярными среди них являются: евклидово расстояние, косинусная мера, расстояние Хэмминга, коэффициент корреляции Пирсона, коэффициент Танимото, Манхэттенское расстояние и некоторые другие. Определение рекомендаций методом User-User предполагает построение матрицы активности пользователей, каждая строка которой описывает действия конкретного пользователя применительно к конкретному объекту (категория, товар, услуга) на сайте. Действия пользователей могут обозначаться самыми различными способами. Например, это может быть бинарная информация о посещении или не посещении заданного ресурса данным пользователем, частота (или число) пользований ресурса r пользователем u, стоимость или рейтинг, проставленный пользователем u для ресурса r и т.д. Таким образом, каждая строка матрицы активности представляет собой вектор оценок, соответствующих различным категориям товаров (тематический профиль пользователя). Профиль пользователя характеризует степень его интереса к каждой группе товаров. Для каждой пары «пользователь-объект (товар, услуга, действие)» в матрице активности вычисляется мера близости с использованием выбранной метрики. Для поиска рекомендаций конкретному пользователю на основании его поведенческого профиля используются три основных подхода: основанный на соседстве (memory based), основанный на модели (model based) и гибридный подход (hybrid). В современных коммерческих системах наибольшее распространение получили гибридный подход и подход, основанный на использовании моделей (алгоритмы кластеризации, байесовские сети доверия, латентные семантические модели). Для выявления групп пользователей со схожими характеристиками часто используются различные алгоритмы кластеризации.  \n",
    "![image](user-user.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Метод Item-Item исторически появился как альтернатива методу User-User, призванная повысить производительность рекомендательных систем для тех магазинов, где число покупателей существенно превышает количество наименований товаров в каталоге. Первоначально данный метод был предложен компанией Amazon для решения следующих основных проблем подхода User-User: проблема холодного старта и проблема частого обновления данных о пользовательской активности. Проблема холодного старта существенно снижает качество работы рекомендательной системы вследствие отсутствия данных о предпочтениях новых (или мало активных) пользователей. Проблема частого обновления данных о пользовательской активности (в случае компании Amazon речь идет о миллионах покупателей) резко снижает производительность рекомендательной системы в целом. Основная идея метода Item-Item заключается в группировке информационных единиц (товары, услуги, действия) имеющих сходные оценки пользователей (рейтинги). Рекомендации вырабатываются по следующему принципу: пользователю оценившему объект X высоко будет предложен объект Y, который высоко оценили другие пользователи, также высоко оценившие и объект X. Использование метода Item-Item позволяет повысить качество рекомендаций для новых пользователей (нет критической зависимости от данных о пользовательских предпочтениях), а также значительно повышает производительность рекомендательной системы в случае, когда количество пользователей существенно превышает количество объектов (характеристики объектов меняются реже). При этом качество рекомендаций в среднем выше, чем в случае использования подхода, основанного на анализе пользовательских профилей. Для вычисления попарной близости информационных единиц могут использоваться те же метрики, что и в случае с парами «пользователь-объект» (часто используется косинусная или модифицированная косинусная меры). Для поиска рекомендаций на основании матрицы объектов часто используются весовые функции и методы регрессионного анализа. Одним из перспективных методов решения задачи Item-Item является метод Item2Vec. Тем не менее для большинства интернет-магазинов подход, связанный с рекомендациями по рейтингам, слабо применим в силу отсутствия возможности мотивировать пользователей определять рейтинг информационных единиц (покупатели приходят из поисковых систем и товарных каталогов, делают нужную им покупку и уходят, чтобы больше никогда не вернуться). И встает задача, как в таких условиях сделать рекомендацию (информационное предложение), на которую откликнется пользователь.\n",
    "![image](item-item.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом описанный метод класса Item-Item вполне применим для новых (или малоактивных) пользователей. При этом по мере накопления данных о предпочтениях пользователей рекомендуются отдавать большее предпочтение методам класса User-User, которые дают тем более точные предсказания чем более подробны данные о пользовательской активности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный метод наименьших квадратов (ALS) - еще один метод, используемый с моделями нелинейной регрессии, когда есть две зависимые переменные (в нашем случае векторы x и y ). Алгоритм фиксирует один из параметров (пользовательские векторы x ), а для другого оптимально решает (векторы элементов y ) путем минимизации квадратичной формы. Алгоритм чередуется между фиксацией векторов пользователей и обновлением векторов элементов, а также фиксированием векторов элементов и обновлением векторов пользователей до тех пор, пока не будут удовлетворены критерии сходимости.  \n",
    "\n",
    "ALS - это итеративный процесс оптимизации, в котором мы на каждой итерации пытаемся подойти все ближе и ближе к факторизованному представлению наших исходных данных.\n",
    "\n",
    "![image](ALS.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общих чертах, матричная факторизация(а варианты ALS особенно хорошо подходят для работы с наборами данных с неявной обратной связью, т.е. с двоичными матрицами) особенно хороша для огромных наборов данных, потому что она находит скрытые факторы в гораздо более управляемом пространстве(как недостаток, MF является дорогостоящим в производстве, поэтому обычно это делается через определенные промежутки времени).   \n",
    "\n",
    "Но для средних наборов данных(например, несколько сотен тысяч пользователей) модели соседства также могут работать хорошо. В частности, CF на основе элементов, который в большинстве случаев может быть вычислен заранее, поскольку сходство элементов обычно стабильно(учитывая, что пользователей обычно намного больше, чем элементов, они масштабируются лучше, чем CF на основе пользователей). И у них есть другие интересные свойства, например, их можно использовать для обнаружения узких сообществ элементов или приводить к легкой объяснимости результатов рекомендаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Recommendation.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Приведите 3 примера весов (те, которых не было на вебинаре: сумма покупок, количество покупок - неинтересно) user-item матрицы для задачи рекомендаций товаров**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативных или дополнительных «весов» можно использовать:\n",
    "<li>комментарии/отзывы и их количество,\n",
    "<li>сезонность \n",
    "<li>лояльность покупателя \n",
    "<li>количество просмотров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Какие ограничения есть у ALS? (Тип информации, линейность/нелинейность факторов и т д)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Огромный объем сетевого трафика: это основное узкое место всех алгоритмов распределенной матричной факторизации. Поскольку мы отправляем вектор признаков по каждому краю графа, объем данных, отправляемых по сети за одну итерацию, пропорционален #Ratings * #Features (здесь и далее в тексте мы используем # как обозначение для 'количества') . Для 100 миллиардов оценок и 100 двойных функций это приводит к 80 ТБ сетевого трафика на итерацию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Некоторые элементы в наших наборах данных очень популярны, поэтому распределение уровней элементов сильно искажено: это может вызвать проблемы с памятью - каждый элемент получает степень * #Features количество данных. Например, если элемент имеет 100 миллионов известных оценок и используется 100 двойных функций, только этот элемент получит 80 ГБ данных. Элементы большой степени также вызывают узкие места в обработке (поскольку каждая вершина обрабатывается атомарно), и все будут ждать завершения нескольких элементов максимальной степени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Мы рассматривали bm_25_weight. \n",
    "Опишите, как он работает. Как сделать рекомендации только на основе bm_25? (Можно и нужно пользоваться любыми источниками, приложите на них ссылки). Какие еще способы перевзвешивания user-item матрицы Вы знаете / можете предложить (ещё 2-3 способа)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.arange(12).reshape(3,4)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_weight(X, K1=100, B=0.8):\n",
    "    \"\"\" Weighs each row of a sparse matrix X  by BM25 weighting \"\"\"\n",
    "    \n",
    "    # calculate idf per term (user)\n",
    "    X = coo_matrix(X) # матрица переводится в формат матрицы типа np.array, но модифицированный под scipy\n",
    "\n",
    "    N = float(X.shape[0])  # принимаются строки матрицы в формате float\n",
    "    binc = np.bincount(X.col) # подсчитывается количество вхождений каждого объекта в строке\n",
    "    idf = np.log(N) - np.log(1 + binc) # из логарифма по количеству строк матрицы\n",
    "                                 # вычитается логарифт 1 + количество каждого объекта в строке\n",
    "\n",
    "    # calculate length_norm per document (artist)\n",
    "    X_sum = X.sum(axis=1)      # суммтся значения столбцов\n",
    "    row_sums = np.ravel(X_sum) # переводятся в одномерный массив\n",
    "    average_length = row_sums.mean() # выводится среднее значение\n",
    "    length_norm = (1.0 - B) + B * row_sums / average_length # нормализация строк матрицы\n",
    "                                                            # одномерный массив делится на его среднее значение\n",
    "                                                            # после этого умножается на дефолтный вес 0.8 + 0.2\n",
    "    \n",
    "    # weight matrix rows by bm25\n",
    "    X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col] # здесь происходит перевзвешивание \n",
    "                                                                                   # вcей матрицы\n",
    "        \n",
    "    return X, idf, length_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, idf, length_norm = bm25_weight(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.67858765, -1.32620242, -1.94491754,  0.        , -1.38361378,\n",
       "       -1.64467298, -1.90085257,  0.        , -1.56418327, -1.72764747,\n",
       "       -1.88917926])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ru.wikipedia.org/wiki/Okapi_BM25  \n",
    "https://xapian.org/docs/bm25.html  \n",
    "https://github.com/benfred/implicit/blob/master/implicit/nearest_neighbours.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "# utils functions like in webinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор матрицы $c_{ui}$\n",
    "Попробуйте различные варианты матрицы весов (3+ вариантов). Обучите алгоритм для различных $C$. В качестве результата приведите таблицу: матрица весов - результат на train и validation.\n",
    "Сделате качественные выводы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация гипперпараметров\n",
    "Для лучшей матрицы весов из первого задания подберите оптимальные $\\lambda$ и n_factors. Подбор можно делать вручную (цикл в цикле, аналог sklearn.GridSearch, или случайно - sklearn.GridSearch). Или Вы можете воспользоваться библиотеками для автоматического подбора гипперпараметров (любые на Ваш вкус). В качестве результата постройте графики:\n",
    "1. Значение параметра - время обучения \n",
    "2. Значение параметра - качество train, качество validation  \n",
    "\n",
    "Сделайте качественные выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P.S.** Не пишите отписки в качестве выводов. Мне интресены Ваши рассуждения, трудности, с которыми Вы сталкнулись и что-то, что Вас удивило. Если выводы контринтуитивны - напишите об этом, в этом нет ничего страшного!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
