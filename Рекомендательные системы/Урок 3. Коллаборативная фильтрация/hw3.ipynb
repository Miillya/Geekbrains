{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Вспомним прошлый вебинар, мы рассматривали User-User рекомендации и Item-Item рекомендации. Чем они отличаются и чем они похожи? Если есть функция item_item_rec(interaction_matrix). Можно ли использовать эту функцию для user_user_rec?  \n",
    "В чем принципиальные отличия item-item рекомендаций от ALS?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе User-User определяется сходство между пользователями и в качестве рекомендаций пользователю выдается n самых часто покупаемых товаров k наиболее похожими на него покупателями. Для оценки степени схожести пользователей в плане их предпочтений могут использоваться различные функции сходства (метрики). Наиболее популярными среди них являются: евклидово расстояние, косинусная мера, расстояние Хэмминга, коэффициент корреляции Пирсона, коэффициент Танимото, Манхэттенское расстояние и некоторые другие. Определение рекомендаций методом User-User предполагает построение матрицы активности пользователей, каждая строка которой описывает действия конкретного пользователя применительно к конкретному объекту (категория, товар, услуга) на сайте. Действия пользователей могут обозначаться самыми различными способами. Например, это может быть бинарная информация о посещении или не посещении заданного ресурса данным пользователем, частота (или число) пользований ресурса r пользователем u, стоимость или рейтинг, проставленный пользователем u для ресурса r и т.д. Таким образом, каждая строка матрицы активности представляет собой вектор оценок, соответствующих различным категориям товаров (тематический профиль пользователя). Профиль пользователя характеризует степень его интереса к каждой группе товаров. Для каждой пары «пользователь-объект (товар, услуга, действие)» в матрице активности вычисляется мера близости с использованием выбранной метрики. Для поиска рекомендаций конкретному пользователю на основании его поведенческого профиля используются три основных подхода: основанный на соседстве (memory based), основанный на модели (model based) и гибридный подход (hybrid). В современных коммерческих системах наибольшее распространение получили гибридный подход и подход, основанный на использовании моделей (алгоритмы кластеризации, байесовские сети доверия, латентные семантические модели). Для выявления групп пользователей со схожими характеристиками часто используются различные алгоритмы кластеризации.  \n",
    "![image](user-user.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Метод Item-Item исторически появился как альтернатива методу User-User, призванная повысить производительность рекомендательных систем для тех магазинов, где число покупателей существенно превышает количество наименований товаров в каталоге. Первоначально данный метод был предложен компанией Amazon для решения следующих основных проблем подхода User-User: проблема холодного старта и проблема частого обновления данных о пользовательской активности. Проблема холодного старта существенно снижает качество работы рекомендательной системы вследствие отсутствия данных о предпочтениях новых (или мало активных) пользователей. Проблема частого обновления данных о пользовательской активности (в случае компании Amazon речь идет о миллионах покупателей) резко снижает производительность рекомендательной системы в целом. Основная идея метода Item-Item заключается в группировке информационных единиц (товары, услуги, действия) имеющих сходные оценки пользователей (рейтинги). Рекомендации вырабатываются по следующему принципу: пользователю оценившему объект X высоко будет предложен объект Y, который высоко оценили другие пользователи, также высоко оценившие и объект X. Использование метода Item-Item позволяет повысить качество рекомендаций для новых пользователей (нет критической зависимости от данных о пользовательских предпочтениях), а также значительно повышает производительность рекомендательной системы в случае, когда количество пользователей существенно превышает количество объектов (характеристики объектов меняются реже). При этом качество рекомендаций в среднем выше, чем в случае использования подхода, основанного на анализе пользовательских профилей. Для вычисления попарной близости информационных единиц могут использоваться те же метрики, что и в случае с парами «пользователь-объект» (часто используется косинусная или модифицированная косинусная меры). Для поиска рекомендаций на основании матрицы объектов часто используются весовые функции и методы регрессионного анализа. Одним из перспективных методов решения задачи Item-Item является метод Item2Vec. Тем не менее для большинства интернет-магазинов подход, связанный с рекомендациями по рейтингам, слабо применим в силу отсутствия возможности мотивировать пользователей определять рейтинг информационных единиц (покупатели приходят из поисковых систем и товарных каталогов, делают нужную им покупку и уходят, чтобы больше никогда не вернуться). И встает задача, как в таких условиях сделать рекомендацию (информационное предложение), на которую откликнется пользователь.\n",
    "![image](item-item.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом описанный метод класса Item-Item вполне применим для новых (или малоактивных) пользователей. При этом по мере накопления данных о предпочтениях пользователей рекомендуются отдавать большее предпочтение методам класса User-User, которые дают тем более точные предсказания чем более подробны данные о пользовательской активности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из самых популярных подходов к заполнению матрицы взаимодействия - матричная факторизация. Мы пытаемся аппроксимировать матрицу взаимодействия как произведение двух матриц меньших размеров, факторов пользователя и факторов элементов:  \n",
    "![image](interaction_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из классических подходов к матричной факторизации называется альтернативным методом наименьших квадратов или ALS.\n",
    "![image](ALS_2.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный метод наименьших квадратов (ALS) - еще один метод, используемый с моделями нелинейной регрессии, когда есть две зависимые переменные (в нашем случае векторы x и y ). Алгоритм фиксирует один из параметров (пользовательские векторы x ), а для другого оптимально решает (векторы элементов y ) путем минимизации квадратичной формы. Алгоритм чередуется между фиксацией векторов пользователей и обновлением векторов элементов, а также фиксированием векторов элементов и обновлением векторов пользователей до тех пор, пока не будут удовлетворены критерии сходимости.  \n",
    "\n",
    "ALS - это итеративный процесс оптимизации, в котором мы на каждой итерации пытаемся подойти все ближе и ближе к факторизованному представлению наших исходных данных.\n",
    "\n",
    "![image](ALS.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общих чертах, матричная факторизация(а варианты ALS особенно хорошо подходят для работы с наборами данных с неявной обратной связью, т.е. с двоичными матрицами) особенно хороша для огромных наборов данных, потому что она находит скрытые факторы в гораздо более управляемом пространстве(как недостаток, MF является дорогостоящим в производстве, поэтому обычно это делается через определенные промежутки времени).   \n",
    "\n",
    "Но для средних наборов данных(например, несколько сотен тысяч пользователей) модели соседства также могут работать хорошо. В частности, CF на основе элементов, который в большинстве случаев может быть вычислен заранее, поскольку сходство элементов обычно стабильно(учитывая, что пользователей обычно намного больше, чем элементов, они масштабируются лучше, чем CF на основе пользователей). И у них есть другие интересные свойства, например, их можно использовать для обнаружения узких сообществ элементов или приводить к легкой объяснимости результатов рекомендаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Recommendation.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Приведите 3 примера весов (те, которых не было на вебинаре: сумма покупок, количество покупок - неинтересно) user-item матрицы для задачи рекомендаций товаров**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативных или дополнительных «весов» можно использовать:\n",
    "<li>комментарии/отзывы и их количество,\n",
    "<li>сезонность \n",
    "<li>лояльность покупателя \n",
    "<li>количество просмотров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Какие ограничения есть у ALS? (Тип информации, линейность/нелинейность факторов и т д)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Огромный объем сетевого трафика: это основное узкое место всех алгоритмов распределенной матричной факторизации. Поскольку мы отправляем вектор признаков по каждому краю графа, объем данных, отправляемых по сети за одну итерацию, пропорционален #Ratings * #Features(здесь и далее в тексте мы используем # как обозначение для 'количества'). Для 100 миллиардов оценок и 100 двойных функций это приводит к 80 ТБ сетевого трафика на итерацию.</li> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Некоторые элементы в наших наборах данных очень популярны, поэтому распределение уровней элементов сильно искажено: это может вызвать проблемы с памятью - каждый элемент получает степень * #Features количество данных. Например, если элемент имеет 100 миллионов известных оценок и используется 100 двойных функций, только этот элемент получит 80 ГБ данных. Элементы большой степени также вызывают узкие места в обработке(поскольку каждая вершина обрабатывается атомарно), и все будут ждать завершения нескольких элементов максимальной степени.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Мы рассматривали bm_25_weight. \n",
    "Опишите, как он работает. Как сделать рекомендации только на основе bm_25? (Можно и нужно пользоваться любыми источниками, приложите на них ссылки). Какие еще способы перевзвешивания user-item матрицы Вы знаете / можете предложить (ещё 2-3 способа)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okapi BM25 — функция ранжирования, используемая поисковыми системами для упорядочивания документов по их релевантности данному поисковому запросу. Она основывается на вероятностной модели, разработанной в 1970-х и 1980-х годах Стивеном Робертсоном, Карен Спарк Джонс и другими.\n",
    "\n",
    "Сама функция носит название BM25 (BM от англ. best match), но её часто называют «Okapi BM25» по названию поисковой системы Okapi, созданной в Лондонском городском университете в 1980-х и 1990-х годах, в которой эта функция была впервые применена.\n",
    "\n",
    "BM25 и его различные более поздние модификации (например, BM25F) представляют собой современные TF-IDF-подобные функции ранжирования, широко используемые на практике в поисковых системах. В веб-поиске эти функции ранжирования часто входят как компоненты более сложной, часто машинно-обученной, функции ранжирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция ранжирования**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25 — поисковая функция на неупорядоченном множестве термов («мешке слов») и множестве документов, которые она оценивает на основе встречаемости слов запроса в каждом документе, без учёта взаимоотношений между ними (например, близости). Это не одна функция, а семейство функций с различными компонентами и параметрами. Одна из распространенных форм этой функции описана ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть дан запрос ${\\displaystyle Q}$, содержащий слова ${\\displaystyle q_{1},...,q_{n}}$, тогда функция BM25 даёт следующую оценку релевантности документа ${\\displaystyle D}$ запросу ${\\displaystyle Q}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\text{score}}(D,Q)=\\sum _{{i=1}}^{{n}}{\\text{IDF}}(q_{i})\\cdot {\\frac  {f(q_{i},D)\\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\\cdot (1-b+b\\cdot {\\frac  {|D|}{{\\text{avgdl}}}})}},$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где ${\\displaystyle f(q_{i},D)}$ есть частота слова (англ. term frequency, TF) ${\\displaystyle q_{i}}$ в документе ${\\displaystyle D}$, ${\\displaystyle |D|}$ есть длина документа (количество слов в нём), а ${\\displaystyle avgdl}$ — средняя длина документа в коллекции. ${\\displaystyle k_{1}}$ и ${\\displaystyle b}$ — свободные коэффициенты, обычно их выбирают как ${\\displaystyle k_{1}=2.0}$ и ${\\displaystyle b=0.75}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\text{IDF}}(q_{i})$ есть обратная документная частота (англ. inverse document frequency, IDF) слова ${\\displaystyle q_{i}}$. Есть несколько толкований IDF и небольших вариации его формулы. Классически, она определяется как:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log {\\frac  {N}{n(q_{i})}},$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где ${\\displaystyle N}$ есть общее количество документов в коллекции, а ${\\displaystyle n(q_{i})}$ — количество документов, содержащих ${\\displaystyle q_{i}}$. Но чаще применяются «сглаженные» варианты этой формулы, например:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\text{IDF}}(q_{i})=\\log {\\frac  {N-n(q_{i})+0.5}{n(q_{i})+0.5}},$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что вышеуказанная формула IDF имеет следующий недостаток. Для слов, входящих в более чем половину документов из коллекции, значение IDF отрицательно. Таким образом, при наличии любых двух почти идентичных документов, в одном из которых есть слово, а в другом — нет, второй может получить бо́льшую оценку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иными словами, часто встречающиеся слова испортят окончательную оценку документа. Это нежелательно, поэтому во многих приложениях вышеприведённая формула может быть скорректирована следующими способами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Игнорировать вообще все отрицательные слагаемые в сумме (что эквивалентно занесению в стоп-лист и игнорированию всех соответствующих высокочастотных слов);  \n",
    "<li>Налагать на IDF некоторую нижнюю границу ${\\displaystyle \\varepsilon }$: если IDF меньше ${\\displaystyle \\varepsilon }$, то считать её равной ${\\displaystyle \\varepsilon }$.\n",
    "<li>Использовать другую формулу IDF, не принимающую отрицательных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерпретация IDF в теории информации**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Положим, что поисковое слово ${\\displaystyle q}$ встречается в ${\\displaystyle n(q)}$ документах. Тогда случайно выбранный документ ${\\displaystyle D}$ содержит слово с вероятностью ${\\displaystyle {\\frac {n(q)}{N}}}$ (где ${\\displaystyle N}$ есть мощность множества документов в коллекции). В таком случае информационная ценность фразы «${\\displaystyle D}$ содержит ${\\displaystyle q}$» будет такова:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\log {\\frac  {n(q)}{N}}=\\log {\\frac  {N}{n(q)}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь положим, что имеется два поисковых слова ${\\displaystyle q_{1}}$ и ${\\displaystyle q_{2}}$. Если они входят в документ независимо друг от друга, то вероятность обнаружить их в случайно выбранном документе ${\\displaystyle D}$ такова:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\frac  {n(q_{1})}{N}}\\cdot {\\frac  {n(q_{2})}{N}},$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и содержание этого события"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum _{{i=1}}^{{2}}\\log {\\frac  {N}{n(q_{i})}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это примерно то, что выражается компонентой IDF в BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модификации**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>При экстремальных значениях коэффициента ${\\displaystyle b}$ в функции BM25 получаются функции ранжирования, известные под названиями BM11 (при ${\\displaystyle b=1}$ и BM15 (при ${\\displaystyle b=0}).$\n",
    "<li>**BM25F** — модификация BM25, в которой документ рассматривается как совокупность нескольких полей (таких как, например, заголовки, основной текст, ссылочный текст), длины которых независимо нормализуются, и каждому из которых может быть назначена своя степень значимости в итоговой функции ранжирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.arange(12).reshape(3,4)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_weight(X, K1=100, B=0.7):\n",
    "    \"\"\" Weighs each row of a sparse matrix X  by BM25 weighting \"\"\"\n",
    "    \n",
    "    # calculate idf per term (user)\n",
    "    X = coo_matrix(X) # матрица переводится в формат матрицы типа np.array, но модифицированный под scipy\n",
    "\n",
    "    N = float(X.shape[0])  # принимаются строки матрицы в формате float\n",
    "    binc = np.bincount(X.col) # подсчитывается количество вхождений каждого объекта в строке\n",
    "    idf = np.log(N) - np.log(1 + binc) # из логарифма по количеству строк матрицы\n",
    "                                 # вычитается логарифт 1 + количество каждого объекта в строке\n",
    "\n",
    "    # calculate length_norm per document (artist)\n",
    "    X_sum = X.sum(axis=1)      # суммтся значения столбцов\n",
    "    row_sums = np.ravel(X_sum) # переводятся в одномерный массив\n",
    "    average_length = row_sums.mean() # выводится среднее значение\n",
    "    length_norm = (1.0 - B) + B * row_sums / average_length # нормализация строк матрицы\n",
    "                                                            # одномерный массив делится на его среднее значение\n",
    "                                                            # после этого умножается на дефолтный вес 0.8 + 0.2\n",
    "    \n",
    "    # weight matrix rows by bm25\n",
    "    X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col] # здесь происходит перевзвешивание \n",
    "                                                                                   # вcей матрицы\n",
    "        \n",
    "    return X, idf, length_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, idf, length_norm = bm25_weight(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58006313, -1.13741915, -1.67337582,  0.        , -1.38361378,\n",
       "       -1.64467298, -1.90085257,  0.        , -1.63532293, -1.80573323,\n",
       "       -1.97403852])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Источники:**  \n",
    "https://ru.wikipedia.org/wiki/Okapi_BM25  \n",
    "https://xapian.org/docs/bm25.html  \n",
    "https://github.com/benfred/implicit/blob/master/implicit/nearest_neighbours.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "# utils functions like in webinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор матрицы $c_{ui}$\n",
    "Попробуйте различные варианты матрицы весов (3+ вариантов). Обучите алгоритм для различных $C$. В качестве результата приведите таблицу: матрица весов - результат на train и validation.\n",
    "Сделате качественные выводы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация гипперпараметров\n",
    "Для лучшей матрицы весов из первого задания подберите оптимальные $\\lambda$ и n_factors. Подбор можно делать вручную (цикл в цикле, аналог sklearn.GridSearch, или случайно - sklearn.GridSearch). Или Вы можете воспользоваться библиотеками для автоматического подбора гипперпараметров (любые на Ваш вкус). В качестве результата постройте графики:\n",
    "1. Значение параметра - время обучения \n",
    "2. Значение параметра - качество train, качество validation  \n",
    "\n",
    "Сделайте качественные выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P.S.** Не пишите отписки в качестве выводов. Мне интресены Ваши рассуждения, трудности, с которыми Вы сталкнулись и что-то, что Вас удивило. Если выводы контринтуитивны - напишите об этом, в этом нет ничего страшного!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
