{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Копия блокнота \"word2vec.ipynb\"","provenance":[{"file_id":"1fuvkK7Pl7ygfiXjiyOlrsOuU3Iw2dAow","timestamp":1617174571213}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"haJUNjSB60Kh"},"source":["# Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"99d4ky2lWFvn"},"source":["Word2Vec - это не единичный алгоритм, а скорее семейство архитектур моделей и оптимизаций, которые можно использовать для изучения встраивания слов из больших наборов данных. Встраивание, полученное с помощью Word2Vec, оказалось успешным в различных последующих задачах обработки естественного языка.\n","\n","Примечание. Это руководство основано на методе «Эффективная оценка представлений слов в векторном пространстве» и « Распределенные представления слов и фраз и их композиционности» . Это не точная реализация документов. Скорее, он предназначен для иллюстрации основных идей.\n","В этих статьях предлагалось два метода изучения представлений слов:\n","\n","Модель непрерывного набора слов, которая предсказывает среднее слово на основе окружающих контекстных слов. Контекст состоит из нескольких слов до и после текущего (среднего) слова. Эта архитектура называется моделью набора слов, поскольку порядок слов в контексте не важен.\n","Модель непрерывной скип-граммы, которая предсказывает слова в определенном диапазоне до и после текущего слова в одном предложении. Рабочий пример этого приведен ниже.\n","В этом руководстве вы будете использовать метод пропуска граммов. Во-первых, вы изучите скип-граммы и другие концепции, используя для иллюстрации одно предложение. Затем вы обучите свою собственную модель Word2Vec на небольшом наборе данных. Это руководство также содержит код для экспорта обученных внедрений и их визуализации в проекторе встраивания TensorFlow ."]},{"cell_type":"markdown","metadata":{"id":"xP00WlaMWBZC"},"source":["## Skip-gram and Negative Sampling "]},{"cell_type":"markdown","metadata":{"id":"Zr2wjv0bW236"},"source":["В то время как модель набора слов предсказывает слово с учетом соседнего контекста, модель пропуска грамматики предсказывает контекст (или соседей) слова с учетом самого слова. Модель обучается на скип-граммах, которые представляют собой n-граммы, которые позволяют пропускать токены (см. Пример на диаграмме ниже). Контекст слова может быть представлен через набор пар пропуска грамматики (target_word, context_word) где context_word появляется в соседнем контексте target_word ."]},{"cell_type":"markdown","metadata":{"id":"ICjc-McbaVTd"},"source":["Рассмотрим следующее предложение из 8 слов.\n","\n","Широкая дорога сияла на палящем солнце.\n","\n","Контекстные слова для каждого из 8 слов этого предложения определяются размером окна. Размер окна определяет диапазон слов по обе стороны от target_word которые можно рассматривать как context word . Взгляните на эту таблицу скип-граммов для целевых слов, основанных на разных размерах окна.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YKE87IKT_YT8"},"source":["Примечание. В этом руководстве размер окна n подразумевает n слов с каждой стороны с общим размером окна 2 * n + 1 слово в слове.\n"]},{"cell_type":"markdown","metadata":{"id":"RsCwQ07E8mqU"},"source":["![word2vec_skipgrams](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/word2vec_skipgram.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"gK1gN1jwkMpU"},"source":["Целью обучения модели пропуска грамматики является максимизация вероятности предсказания контекстных слов с учетом целевого слова. Для последовательности слов *w<sub>1</sub>, w<sub>2</sub> , ... w<sub>T</sub>* цель может быть записана как средняя логарифмическая вероятность\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pILO_iAc84e-"},"source":["![word2vec_skipgram_objective](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/word2vec_skipgram_objective.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"Gsy6TUbtnz_K"},"source":["где c - размер обучающего контекста. Базовая формулировка скип-граммы определяет эту вероятность с помощью функции softmax."]},{"cell_type":"markdown","metadata":{"id":"P81Qavbb9APd"},"source":["![word2vec_full_softmax](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/word2vec_full_softmax.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"axZvd-hhotVB"},"source":["где v и v ' - целевые и контекстные векторные представления слов, а W - размер словаря.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SoLzxbqSpT6_"},"source":["Вычисление знаменателя этой формулировки включает выполнение полного softmax для всего словарного запаса, который часто бывает большим (10^5 -10^7 ) терминами.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y5VWYtmFzHkU"},"source":["Функция потерь с контрастной оценкой шума является эффективным приближением для полного softmax. С целью изучения встраивания слов вместо моделирования распределения слов, потерю NCE можно упростить, используя отрицательную выборку.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WTZBPf1RsOsg"},"source":["Упрощенная цель отрицательной выборки для целевого слова состоит в том, чтобы отличить контекстное слово от отрицательных выборок num_ns, взятых из распределения шума P <sub>n</sub> (w) слов. Точнее, эффективное приближение полного softmax по словарю для пары пропуска-грамма представляет собой потерю целевого слова как проблему классификации между контекстным словом и отрицательными выборками num_ns ."]},{"cell_type":"markdown","metadata":{"id":"Cl0rSfHjt6Mf"},"source":["Отрицательный образец определяется как пара (target_word, context_word), так что context_word не появляется в окрестности window_size для target_word. Для примера предложения это несколько потенциальных отрицательных выборок (когда window_size равно 2).\n","\n","```\n","(hot, shimmered)\n","(wide, hot)\n","(wide, sun)\n","```"]},{"cell_type":"markdown","metadata":{"id":"kq0q2uqbucFg"},"source":["В следующем разделе вы создадите скип-граммы и отрицательные образцы для одного предложения. Вы также узнаете о методах подвыборки и обучите модель классификации для положительных и отрицательных обучающих примеров позже в этом руководстве."]},{"cell_type":"markdown","metadata":{"id":"mk4-Hpe1CH16"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"OSUalgiCchft"},"source":["!pip install tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RutaI-Tpev3T"},"source":["import io\n","import itertools\n","import numpy as np\n","import os\n","import re\n","import string\n","import tensorflow as tf\n","import tqdm\n","\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkJ5299Tek6B"},"source":["SEED = 42 \n","AUTOTUNE = tf.data.experimental.AUTOTUNE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RW-g5buCHwh3"},"source":["### Vectorize an example sentence"]},{"cell_type":"markdown","metadata":{"id":"y8TfZIgoQrcP"},"source":["Рассмотрим следующее предложение:    \n","`The wide road shimmered in the hot sun.`\n","\n","Обозначьте предложение:"]},{"cell_type":"code","metadata":{"id":"bsl7jBzV6_KK"},"source":["sentence = \"The wide road shimmered in the hot sun\"\n","tokens = list(sentence.lower().split())\n","print(len(tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PU-bs1XtThEw"},"source":["Создайте словарь, чтобы сохранять сопоставления токенов с целочисленными индексами.\n","\n"]},{"cell_type":"code","metadata":{"id":"UdYv1HJUQ8XA"},"source":["vocab, index = {}, 1 # start indexing from 1\n","vocab['<pad>'] = 0 # add a padding token \n","for token in tokens:\n","  if token not in vocab: \n","    vocab[token] = index\n","    index += 1\n","vocab_size = len(vocab)\n","print(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpuP43Dddasr"},"source":["Создайте обратный словарь, чтобы сохранить сопоставления от целочисленных индексов к токенам.\n","\n"]},{"cell_type":"code","metadata":{"id":"o9ULAJYtEvKl"},"source":["inverse_vocab = {index: token for token, index in vocab.items()}\n","print(inverse_vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3qtuyxIRyii"},"source":["Векторизуйте свое предложение."]},{"cell_type":"code","metadata":{"id":"CsB3-9uQQYyl"},"source":["example_sequence = [vocab[word] for word in tokens]\n","print(example_sequence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ox1I28JRIOdM"},"source":["### Создавайте скип-граммы из одного предложения"]},{"cell_type":"markdown","metadata":{"id":"t7NNKAmSiHvy"},"source":["Модуль tf.keras.preprocessing.sequence предоставляет полезные функции, упрощающие подготовку данных для Word2Vec. Вы можете использовать tf.keras.preprocessing.sequence.skipgrams для генерации пар пропуска-грамм из example_sequence с заданным window_size из токенов в диапазоне [0, vocab_size)\n","\n","Примечание. Для параметра negative_samples установлено значение 0 поскольку для пакетной обработки отрицательных выборок, сгенерированных этой функцией, требуется немного кода. В следующем разделе вы будете использовать другую функцию для выполнения отрицательной выборки.\n"]},{"cell_type":"code","metadata":{"id":"USAJxW4RD7pn"},"source":["window_size = 2\n","positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","      example_sequence, \n","      vocabulary_size=vocab_size,\n","      window_size=window_size,\n","      negative_samples=0)\n","print(len(positive_skip_grams))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uc9uhiMwY-AQ"},"source":["Взгляните на несколько положительных скип-грамм."]},{"cell_type":"code","metadata":{"id":"SCnqEukIE9pt"},"source":["for target, context in positive_skip_grams[:5]:\n","  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ua9PkMTISF0"},"source":["### Отрицательная выборка на один скип-грамм"]},{"cell_type":"markdown","metadata":{"id":"Esqn8WBfZnEK"},"source":["Функция skipgrams возвращает все положительные пары skip-gram путем скольжения по заданному промежутку окна. Чтобы создать дополнительные пары скип-грамм, которые будут служить отрицательными образцами для обучения, вам необходимо выбрать случайные слова из словаря. Используйте функцию tf.random.log_uniform_candidate_sampler для выборки числа отрицательных выборок num_ns для данного целевого слова в окне. Вы можете вызвать функцию для целевого слова одной скип-граммы и передать контекстное слово как истинный класс, чтобы исключить его из выборки.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AgH3aSvw3xTD"},"source":["Ключевой момент: num_ns (количество отрицательных выборок на одно положительное контекстное слово) между [5, 20], как показано, лучше всего работает для небольших наборов данных, тогда как num_ns между [2,5] достаточно для больших наборов данных.\n"]},{"cell_type":"code","metadata":{"id":"m_LmdzqIGr5L"},"source":["# Get target and context words for one positive skip-gram.\n","target_word, context_word = positive_skip_grams[0]\n","\n","# Set the number of negative samples per positive context. \n","num_ns = 4\n","\n","context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n","negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","    true_classes=context_class, # class that should be sampled as 'positive'\n","    num_true=1, # each positive skip-gram has 1 positive context class\n","    num_sampled=num_ns, # number of negative context words to sample\n","    unique=True, # all the negative samples should be unique\n","    range_max=vocab_size, # pick index of the samples from [0, vocab_size]\n","    seed=SEED, # seed for reproducibility\n","    name=\"negative_sampling\" # name of this operation\n",")\n","print(negative_sampling_candidates)\n","print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8MSxWCrLIalp"},"source":["### Создайте один обучающий пример"]},{"cell_type":"markdown","metadata":{"id":"Q6uEWdj8vKKv"},"source":["Для данной положительной (target_word, context_word) skip-граммы теперь у вас также есть num_ns отрицательных выбранных контекстных слов, которые не появляются в окрестности размера окна target_word . Пакетный в 1 положительных context_word и num_ns отрицательные контекстные слова в один тензор. Это дает набор положительных скип-грамм (помеченных как 1 ) и отрицательных выборок (помеченных как 0 ) для каждого целевого слова.\n","\n"]},{"cell_type":"code","metadata":{"id":"zSiZwifuLvHf"},"source":["# Add a dimension so you can use concatenation (on the next step).\n","negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n","\n","# Concat positive context word with negative sampled words.\n","context = tf.concat([context_class, negative_sampling_candidates], 0)\n","\n","# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n","label = tf.constant([1] + [0]*num_ns, dtype=\"int64\") \n","\n","# Reshape target to shape (1,) and context and label to (num_ns+1,).\n","target = tf.squeeze(target_word)\n","context = tf.squeeze(context)\n","label =  tf.squeeze(label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIJeoFCAwtXJ"},"source":["Взгляните на контекст и соответствующие ярлыки для целевого слова из приведенного выше примера пропуска граммы.\n"]},{"cell_type":"code","metadata":{"id":"tzyCPCuZwmdL"},"source":["print(f\"target_index    : {target}\")\n","print(f\"target_word     : {inverse_vocab[target_word]}\")\n","print(f\"context_indices : {context}\")\n","print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n","print(f\"label           : {label}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gBtTcUVQr8EO"},"source":["Кортеж тензоров (target, context, label) составляет один обучающий пример для обучения вашей модели Word2Vec с отрицательной выборкой с пропуском грамматики. Обратите внимание, что цель имеет форму (1,) а контекст и метка имеют форму (1+num_ns,)\n","\n"]},{"cell_type":"code","metadata":{"id":"x-FwkR8jx9-Z"},"source":["print(f\"target  :\", target)\n","print(f\"context :\", context )\n","print(f\"label   :\", label )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bRJIlow4Dlv"},"source":["### Summary"]},{"cell_type":"markdown","metadata":{"id":"pWkuha0oykG5"},"source":["На этом рисунке изображена процедура создания обучающего примера из предложения."]},{"cell_type":"markdown","metadata":{"id":"_KlwdiAa9crJ"},"source":["![word2vec_negative_sampling](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/word2vec_negative_sampling.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"9wmdO_MEIpaM"},"source":["## Скомпилируйте все шаги в одну функцию\n"]},{"cell_type":"markdown","metadata":{"id":"iLKwNAczHsKg"},"source":["### Таблица отбора проб на скип-грамм"]},{"cell_type":"markdown","metadata":{"id":"TUUK3uDtFNFE"},"source":["Большой набор данных означает больший словарный запас с большим количеством более частых слов, таких как стоп-слова. Примеры обучения , полученные от выборки , обычно происходящие слов (например, , the is , on ) не добавляют много полезной информации для модели узнать. Миколов и др. предложите подвыборку часто встречающихся слов в качестве полезной практики для улучшения качества встраивания.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bPtbv7zNP7Dx"},"source":["Функция tf.keras.preprocessing.sequence.skipgrams принимает аргумент таблицы выборки для кодирования вероятностей выборки любого токена. Вы можете использовать tf.keras.preprocessing.sequence.make_sampling_table для создания таблицы вероятностной выборки на основе частотного ранга и передать ее функции skipgrams . Взгляните на вероятности выборки для параметра vocab_size 10.\n","\n"]},{"cell_type":"code","metadata":{"id":"Rn9zAnDccyRg"},"source":["sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n","print(sampling_table)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EHvSptcPk5fp"},"source":["sampling_table[i] обозначает вероятность выборки i-го наиболее распространенного слова в наборе данных. Функция предполагает распределение частот слов по Ципфу для выборки."]},{"cell_type":"markdown","metadata":{"id":"mRHMssMmHgH-"},"source":["Ключевой момент: tf.random.log_uniform_candidate_sampler уже предполагает, что частота словарного запаса соответствует логарифмически равномерному (Zipf's) распределению. Использование этой выборки, взвешенной по распределению, также помогает аппроксимировать потерю контрастной оценки шума (NCE) с помощью более простых функций потерь для обучения отрицательной цели выборки.\n"]},{"cell_type":"markdown","metadata":{"id":"aj--8RFK6fgW"},"source":["### Создавать обучающие данные"]},{"cell_type":"markdown","metadata":{"id":"dy5hl4lQ0B2M"},"source":["Скомпилируйте все шаги, описанные выше, в функцию, которую можно вызвать для списка векторизованных предложений, полученных из любого набора текстовых данных. Обратите внимание, что таблица выборки создается до выборки пар слов с пропуском грамма. Вы будете использовать эту функцию в следующих разделах.\n","\n"]},{"cell_type":"code","metadata":{"id":"63INISDEX1Hu"},"source":["# Generates skip-gram pairs with negative sampling for a list of sequences\n","# (int-encoded sentences) based on window size, number of negative samples\n","# and vocabulary size.\n","def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n","  # Elements of each training example are appended to these lists.\n","  targets, contexts, labels = [], [], []\n","\n","  # Build the sampling table for vocab_size tokens.\n","  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","  # Iterate over all sequences (sentences) in dataset.\n","  for sequence in tqdm.tqdm(sequences):\n","\n","    # Generate positive skip-gram pairs for a sequence (sentence).\n","    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","          sequence, \n","          vocabulary_size=vocab_size,\n","          sampling_table=sampling_table,\n","          window_size=window_size,\n","          negative_samples=0)\n","    \n","    # Iterate over each positive skip-gram pair to produce training examples \n","    # with positive context word and negative samples.\n","    for target_word, context_word in positive_skip_grams:\n","      context_class = tf.expand_dims(\n","          tf.constant([context_word], dtype=\"int64\"), 1)\n","      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","          true_classes=context_class,\n","          num_true=1, \n","          num_sampled=num_ns, \n","          unique=True, \n","          range_max=vocab_size, \n","          seed=SEED, \n","          name=\"negative_sampling\")\n","      \n","      # Build context and label vectors (for one target word)\n","      negative_sampling_candidates = tf.expand_dims(\n","          negative_sampling_candidates, 1)\n","\n","      context = tf.concat([context_class, negative_sampling_candidates], 0)\n","      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","      # Append each element from the training example to global lists.\n","      targets.append(target_word)\n","      contexts.append(context)\n","      labels.append(label)\n","\n","  return targets, contexts, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shvPC8Ji2cMK"},"source":["## Подготовить обучающие данные для Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"j5mbZsZu6uKg"},"source":["Зная, как работать с одним предложением для модели Word2Vec на основе отрицательной выборки с пропуском грамм, вы можете приступить к созданию обучающих примеров из большего списка предложений!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OFlikI6L26nh"},"source":["### Download text corpus\n"]},{"cell_type":"markdown","metadata":{"id":"rEFavOgN98al"},"source":["Для этого урока вы будете использовать текстовый файл, написанный Шекспиром. Измените следующую строку, чтобы запустить этот код на ваших собственных данных.\n","\n"]},{"cell_type":"code","metadata":{"id":"QFkitxzVVaAi"},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sOsbLq8a37dr"},"source":["Прочтите текст из файла и посмотрите на первые несколько строк.\n","\n"]},{"cell_type":"code","metadata":{"id":"lfgnsUw3ofMD"},"source":["with open(path_to_file) as f: \n","  lines = f.read().splitlines()\n","for line in lines[:20]:\n","  print(line)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTNZYqUs5C2V"},"source":["Используйте непустые строки для tf.data.TextLineDataset объекта tf.data.TextLineDataset для следующих шагов.\n","\n"]},{"cell_type":"code","metadata":{"id":"ViDrwy-HjAs9"},"source":["text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfsc88zE9upk"},"source":["### Векторизовать предложения из корпуса"]},{"cell_type":"markdown","metadata":{"id":"XfgZo8zR94KK"},"source":["Вы можете использовать слой TextVectorization для векторизации предложений из корпуса. Узнайте больше об использовании этого слоя в этом руководстве по классификации текста . Обратите внимание на несколько первых предложений выше, что текст должен быть в одном регистре и пунктуация должна быть удалена. Для этого определите функцию custom_standardization function которую можно использовать в слое TextVectorization.\n","\n"]},{"cell_type":"code","metadata":{"id":"2MlsXzo-ZlfK"},"source":["# We create a custom standardization function to lowercase the text and \n","# remove punctuation.\n","def custom_standardization(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  return tf.strings.regex_replace(lowercase,\n","                                  '[%s]' % re.escape(string.punctuation), '')\n","\n","# Define the vocabulary size and number of words in a sequence.\n","vocab_size = 4096\n","sequence_length = 10\n","\n","# Use the text vectorization layer to normalize, split, and map strings to\n","# integers. Set output_sequence_length length to pad all samples to same length.\n","vectorize_layer = TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g92LuvnyBmz1"},"source":["Для создания словаря вызовите adapt текстового набора данных.\n","\n"]},{"cell_type":"code","metadata":{"id":"seZau_iYMPFT"},"source":["vectorize_layer.adapt(text_ds.batch(1024))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jg2z7eeHMnH-"},"source":["После того, как состояние слоя было адаптировано для представления текстового корпуса, словарь может быть доступен с помощью get_vocabulary() . Эта функция возвращает список всех лексем лексики, отсортированных (по убыванию) по их частоте.\n","\n"]},{"cell_type":"code","metadata":{"id":"jgw9pTA7MRaU"},"source":["# Save the created vocabulary for reference.\n","inverse_vocab = vectorize_layer.get_vocabulary()\n","print(inverse_vocab[:20])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DOQ30Tx6KA2G"},"source":["Vectorize_layer теперь можно использовать для генерации векторов для каждого элемента в text_ds ."]},{"cell_type":"code","metadata":{"id":"yUVYrDp0araQ"},"source":["def vectorize_text(text):\n","  text = tf.expand_dims(text, -1)\n","  return tf.squeeze(vectorize_layer(text))\n","\n","# Vectorize the data in text_ds.\n","text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7YyH_SYzB72p"},"source":["### Получить последовательности из набора данных"]},{"cell_type":"markdown","metadata":{"id":"NFUQLX0_KaRC"},"source":["Теперь у вас естьtf.data.Dataset целочисленных предложений. Чтобы подготовить набор данных для обучения модели Word2Vec, сведите набор данных в список последовательностей векторов предложений. Этот шаг необходим, поскольку вы будете перебирать каждое предложение в наборе данных для получения положительных и отрицательных примеров. \n","\n","Примечание. Поскольку функция generate_training_data() определенная ранее, использует функции python / numpy, отличные от TF, вы также можете использовать tf.py_function или tf.numpy_function с tf.data.Dataset.map() .\n"]},{"cell_type":"code","metadata":{"id":"sGXoOh9y11pM"},"source":["sequences = list(text_vector_ds.as_numpy_iterator())\n","print(len(sequences))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDc4riukLTqg"},"source":["Взгляните на несколько примеров из sequences ."]},{"cell_type":"code","metadata":{"id":"WZf1RIbB2Dfb"},"source":["for seq in sequences[:5]:\n","  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDzSOjNwCWNh"},"source":["### Создавайте обучающие примеры из последовательностей"]},{"cell_type":"markdown","metadata":{"id":"BehvYr-nEKyY"},"source":["sequences теперь является список предложений в кодировке int. Просто вызовите функцию generate_training_data() определенную ранее, чтобы сгенерировать обучающие примеры для модели Word2Vec. Напомним, функция выполняет итерацию по каждому слову из каждой последовательности для сбора положительных и отрицательных контекстных слов. Длина цели, контексты и метки должны быть одинаковыми, представляя общее количество обучающих примеров."]},{"cell_type":"code","metadata":{"id":"44DJ22M6nX5o"},"source":["targets, contexts, labels = generate_training_data(\n","    sequences=sequences, \n","    window_size=2, \n","    num_ns=4, \n","    vocab_size=vocab_size, \n","    seed=SEED)\n","print(len(targets), len(contexts), len(labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"97PqsusOFEpc"},"source":["### Настройте набор данных для повышения производительности"]},{"cell_type":"markdown","metadata":{"id":"7jnFVySViQTj"},"source":["Чтобы выполнить эффективную пакетную обработку потенциально большого количества обучающих примеров, используйтеtf.data.Dataset API. После этого шага у вас будет объектtf.data.Dataset из элементов (target_word, context_word), (label) для обучения вашей модели Word2Vec!\n"]},{"cell_type":"code","metadata":{"id":"nbu8PxPSnVY2"},"source":["BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tyrNX6Fs6K3F"},"source":["Добавьте cache() и prefetch() для повышения производительности.\n","\n"]},{"cell_type":"code","metadata":{"id":"Y5Ueg6bcFPVL"},"source":["dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1S-CmUMszyEf"},"source":["## Model and Training"]},{"cell_type":"markdown","metadata":{"id":"sQFqaBMPwBqC"},"source":["Модель Word2Vec может быть реализована как классификатор, позволяющий отличать истинные контекстные слова от пропущенных граммов и ложные контекстные слова, полученные с помощью отрицательной выборки. Вы можете выполнить скалярное произведение между вложениями целевого и контекстного слов, чтобы получить прогнозы для меток и вычислить потери по сравнению с истинными метками в наборе данных.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oc7kTbiwD9sy"},"source":["### Подкласс модели Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"Jvr9pM1G1sQN"},"source":["Используйте API подклассов Keras, чтобы определить модель Word2Vec со следующими слоями:\n","\n","target_embedding : слой tf.keras.layers.Embedding который ищет встраивание слова, когда оно появляется как целевое слово. Количество параметров в этом слое: (vocab_size * embedding_dim) .\n","context_embedding : еще tf.keras.layers.Embedding слой tf.keras.layers.Embedding который ищет встраивание слова, когда оно появляется как контекстное слово. Количество параметров в этом слое такое же, как и в target_embedding , то есть (vocab_size * embedding_dim) .\n","dots : слой tf.keras.layers.Dot который вычисляет скалярное произведение целевых и контекстных встраиваний из обучающей пары.\n","flatten : а tf.keras.layers.Flatten слой , чтобы сгладить результаты dots слоя в логит.\n","С помощью модели с подгруппами вы можете определить функцию call() которая принимает пары (target, context) которые затем могут быть переданы на соответствующий уровень внедрения. Перерисуйте context_embedding выполнить скалярное произведение с target_embedding и вернуть сплюснутый результат.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KiAwuIqqw7-7"},"source":["Ключевой пункт: target_embedding и context_embedding слои могут быть разделены , а также. Вы также можете использовать объединение обоих вложений в качестве окончательного вложения Word2Vec.\n"]},{"cell_type":"code","metadata":{"id":"i9ec-sS6xd8Z"},"source":["class Word2Vec(Model):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(Word2Vec, self).__init__()\n","    self.target_embedding = Embedding(vocab_size, \n","                                      embedding_dim,\n","                                      input_length=1,\n","                                      name=\"w2v_embedding\", )\n","    self.context_embedding = Embedding(vocab_size, \n","                                       embedding_dim, \n","                                       input_length=num_ns+1)\n","    self.dots = Dot(axes=(3,2))\n","    self.flatten = Flatten()\n","\n","  def call(self, pair):\n","    target, context = pair\n","    we = self.target_embedding(target)\n","    ce = self.context_embedding(context)\n","    dots = self.dots([ce, we])\n","    return self.flatten(dots)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RLKz9LFECXu"},"source":["### Определите функцию потерь и скомпилируйте модель\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I3Md-9QanqBM"},"source":["Для простоты вы можете использовать tf.keras.losses.CategoricalCrossEntropy в качестве альтернативы отрицательной потере выборки. Если вы хотите написать свою собственную функцию потерь, вы также можете сделать это следующим образом:\n","\n","\n","``` python\n","def custom_loss(x_logit, y_true):\n","      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n","```\n","\n","Пришло время построить свою модель! Создайте экземпляр своего класса Word2Vec с размером встраивания 128 (вы можете поэкспериментировать с разными значениями). Скомпилируйте модель с tf.keras.optimizers.Adam оптимизатора tf.keras.optimizers.Adam . "]},{"cell_type":"code","metadata":{"id":"ekQg_KbWnnmQ"},"source":["embedding_dim = 128\n","word2vec = Word2Vec(vocab_size, embedding_dim)\n","word2vec.compile(optimizer='adam',\n","              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3MUMrluqNX2"},"source":["Также определите обратный вызов для регистрации статистики обучения для тензорной доски.\n","\n"]},{"cell_type":"code","metadata":{"id":"9d-ftBCeEZIR"},"source":["tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5wEBotlGZ7B"},"source":["Обучите модель с помощью dataset подготовленного выше для некоторого количества эпох.\n","\n"]},{"cell_type":"code","metadata":{"id":"gmC1BJalEZIY"},"source":["word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wze38jG57XvZ"},"source":["Tensorboard теперь показывает точность и потери модели Word2Vec.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i0VuZcvm8zaI"},"source":["```python\n","%tensorboard --logdir logs\n","```"]},{"cell_type":"markdown","metadata":{"id":"awF3iRQCZOLj"},"source":["![word2vec_tensorboard](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/word2vec_tensorboard.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"TaDW2tIIz8fL"},"source":["## Встраивание поиска и анализа"]},{"cell_type":"markdown","metadata":{"id":"Zp5rv01WG2YA"},"source":["Получите веса из модели с помощью get_layer() и get_weights() . Функция get_vocabulary() предоставляет словарь для создания файла метаданных с одним токеном на строку.\n","\n"]},{"cell_type":"code","metadata":{"id":"_Uamp1YH8RzU"},"source":["weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gWzdmUzS8Sl4"},"source":["Создайте и сохраните файлы векторов и метаданных.\n"]},{"cell_type":"code","metadata":{"id":"VLIahl9s53XT"},"source":["out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n","out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n","\n","for index, word in enumerate(vocab):\n","  if  index == 0: continue # skip 0, it's padding.\n","  vec = weights[index] \n","  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","  out_m.write(word + \"\\n\")\n","out_v.close()\n","out_m.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1T8KcThhIU8-"},"source":["Загрузите vectors.tsv и metadata.tsv чтобы проанализировать полученные вложения в проекторе вложений."]},{"cell_type":"code","metadata":{"id":"lUsjQOKMIV2z"},"source":["try:\n","  from google.colab import files\n","  files.download('vectors.tsv')\n","  files.download('metadata.tsv')\n","except Exception as e:\n","  pass"],"execution_count":null,"outputs":[]}]}