{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dried-louisville",
   "metadata": {
    "id": "dried-louisville"
   },
   "source": [
    "### Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ortbQg9AP2m4",
   "metadata": {
    "id": "ortbQg9AP2m4"
   },
   "outputs": [],
   "source": [
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-court",
   "metadata": {
    "id": "unsigned-court"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-guatemala",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1622017926369,
     "user": {
      "displayName": "Alexey Tankov",
      "photoUrl": "",
      "userId": "01729535101672247421"
     },
     "user_tz": -180
    },
    "id": "phantom-guatemala",
    "outputId": "1c792b0c-764a-4c4f-f459-e47ac195ee4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:72.5% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:72.5% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cordless-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a805da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "pd.set_option('precision', 3)\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3185cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#     raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985f75f",
   "metadata": {},
   "source": [
    "### Paths to directories and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAFT_DATASET_PATH = 'E:/kaggle/mailru/Otvety.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec970111",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "destroyed-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = io.open(DRAFT_DATASET_PATH, encoding='UTF-8').read()#.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-resistance",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11da6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(w):\n",
    "    w = re.sub(':\\)', '', w)\n",
    "    w = re.sub('[)\"]', '', w)\n",
    "    w = re.sub('<[^>]+>', ' ', w)\n",
    "    \n",
    "    w = re.sub('\\s*\\?\\s*\\.', '?', w)\n",
    "    w = re.sub('\\s*\\!\\s*\\.', '!', w)\n",
    "    w = re.sub('\\s*\\.', '.', w)\n",
    "    w = re.sub('\\.+', '.', w)\n",
    "    \n",
    "#     w = w.lower().strip()\n",
    "    w = re.sub('---', 'QUESTION', w)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c5686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072593541\n"
     ]
    }
   ],
   "source": [
    "# print(type(lines))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4589fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION\n",
      "вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести? \n",
      "хомячка. \n",
      "мужика, йопаря, собачку и 50 кошек. \n",
      "Общение! \n",
      "паучка. \n",
      "Да пол мне бы памыть! А таг то ни чо. Типа ни каво! \n",
      "я тут вообще что бы пообщаться. \n",
      "А мне советовали сиси завести. \n",
      "Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали. \n",
      "мне тут советовали завести любовника, мужа и много кошек  приветик. \n",
      "QUESTION\n",
      "Как парни относятся к цветным линзам? Если у девушки то зеленые глаза, то голубые. \n",
      "меня вобще прикалывает эта тема. \n",
      "когда этобыло редкость - было забавно, а когда все знают, что эта фальшивка, то уже не прикольно, как силиконовые сиськи или как налепленные синтетические волосы. \n",
      "QUESTION\n",
      "Что делать, сегодня нашёл 2 миллиона рублей? \n",
      "Если это счастье  действительно на вас свалилось, лучше пойти в милицию и заявить о находке. Такие деньги просто так не терют, а что самое интересное их неприменно будут искать и поверьте мне найдут, видел подобное в жизни\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 500000\n",
    "text = preprocess_text(str(lines[:NUM_EXAMPLES]))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "little-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_sentence(sent):\n",
    "    sent = text.split('\\nQUESTION\\n')[1:]\n",
    "    \n",
    "    question = []\n",
    "    answer = []\n",
    "\n",
    "    for se in sent:\n",
    "        se = se.split('\\n')\n",
    "        question.append(se[0].strip())\n",
    "#         answer.append(' '.join([f'<start_answers> {s}<stop_answers>' for s in se[1:]]))\n",
    "        answer.append(' '.join([f' {s}' for s in se[1:]]))\n",
    "    \n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handed-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести?',\n",
       " ' хомячка.   мужика, йопаря, собачку и 50 кошек.   Общение!   паучка.   Да пол мне бы памыть! А таг то ни чо. Типа ни каво!   я тут вообще что бы пообщаться.   А мне советовали сиси завести.   Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали.   мне тут советовали завести любовника, мужа и много кошек  приветик. ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, answer = split_by_sentence(text)\n",
    "question[0], answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c15ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'questions':question, 'answers':answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254e329c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести?</td>\n",
       "      <td>хомячка.   мужика, йопаря, собачку и 50 кошек.   Общение!   паучка.   Да пол мне бы памыть! А таг то ни чо. Типа ни каво!   я тут вообще что бы пообщаться.   А мне советовали сиси завести.   Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали.   мне тут совет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Как парни относятся к цветным линзам? Если у девушки то зеленые глаза, то голубые.</td>\n",
       "      <td>меня вобще прикалывает эта тема.   когда этобыло редкость - было забавно, а когда все знают, что эта фальшивка, то уже не прикольно, как силиконовые сиськи или как налепленные синтетические волосы.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Что делать, сегодня нашёл 2 миллиона рублей?</td>\n",
       "      <td>Если это счастье  действительно на вас свалилось, лучше пойти в милицию и заявить о находке. Такие деньги просто так не терют, а что самое интересное их неприменно будут искать и поверьте мне найдут, видел подобное в жизни. Можно нарваться на бабушку конечно, которая хотела помоч внуку с покупк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Эбу в двенашке называется Итэлма что за эбу?</td>\n",
       "      <td>ЭБУ — электронный блок управления двигателем автомобиля, его другое название — контроллер. Он принимает информацию от многочисленных датчиков, обрабатывает ее по особым алгоритмам и, отталкиваясь от полученных данных, отдает команды исполнительным устройствам системы.   Это завод. А ЭБУ может и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>академия вампиров. сколько на даный момент частей книги академия вампиров?</td>\n",
       "      <td>4. Охотники и Жертвы, Ледяной укус, Поцелуй тьмы, Кровная клятва.   На данное время их 6. часть 5- Оковы для призрака (Духовная связь часть 6-Последняя жертва.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            questions  \\\n",
       "0       вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести?   \n",
       "1  Как парни относятся к цветным линзам? Если у девушки то зеленые глаза, то голубые.   \n",
       "2                                        Что делать, сегодня нашёл 2 миллиона рублей?   \n",
       "3                                        Эбу в двенашке называется Итэлма что за эбу?   \n",
       "4          академия вампиров. сколько на даный момент частей книги академия вампиров?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                       answers  \n",
       "0   хомячка.   мужика, йопаря, собачку и 50 кошек.   Общение!   паучка.   Да пол мне бы памыть! А таг то ни чо. Типа ни каво!   я тут вообще что бы пообщаться.   А мне советовали сиси завести.   Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали.   мне тут совет...  \n",
       "1                                                                                                       меня вобще прикалывает эта тема.   когда этобыло редкость - было забавно, а когда все знают, что эта фальшивка, то уже не прикольно, как силиконовые сиськи или как налепленные синтетические волосы.   \n",
       "2   Если это счастье  действительно на вас свалилось, лучше пойти в милицию и заявить о находке. Такие деньги просто так не терют, а что самое интересное их неприменно будут искать и поверьте мне найдут, видел подобное в жизни. Можно нарваться на бабушку конечно, которая хотела помоч внуку с покупк...  \n",
       "3   ЭБУ — электронный блок управления двигателем автомобиля, его другое название — контроллер. Он принимает информацию от многочисленных датчиков, обрабатывает ее по особым алгоритмам и, отталкиваясь от полученных данных, отдает команды исполнительным устройствам системы.   Это завод. А ЭБУ может и...  \n",
       "4                                                                                                                                             4. Охотники и Жертвы, Ледяной укус, Поцелуй тьмы, Кровная клятва.   На данное время их 6. часть 5- Оковы для призрака (Духовная связь часть 6-Последняя жертва.   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec40647",
   "metadata": {},
   "source": [
    "### BERT Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb7b2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10c79563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "\n",
    "    def preprocess(self):\n",
    "        # clean context and question\n",
    "        context = \" \".join(str(self.context).split())\n",
    "        question = \" \".join(str(self.question).split())\n",
    "        # tokenize context and question\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        # if this is validation or training sample, preprocess answer\n",
    "        if self.answer_text is not None:\n",
    "            answer = \" \".join(str(self.answer_text).split())\n",
    "            # check if end character index is in the context\n",
    "            end_char_idx = self.start_char_idx + len(answer)\n",
    "            if end_char_idx >= len(context):\n",
    "                self.skip = True\n",
    "                return\n",
    "            # mark all the character indexes in context that are also in answer     \n",
    "            is_char_in_ans = [0] * len(context)\n",
    "            for idx in range(self.start_char_idx, end_char_idx):\n",
    "                is_char_in_ans[idx] = 1\n",
    "            ans_token_idx = []\n",
    "            # find all the tokens that are in the answers\n",
    "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "                if sum(is_char_in_ans[start:end]) > 0:\n",
    "                    ans_token_idx.append(idx)\n",
    "            if len(ans_token_idx) == 0:\n",
    "                self.skip = True\n",
    "                return\n",
    "            # get start and end token indexes\n",
    "            self.start_token_idx = ans_token_idx[0]\n",
    "            self.end_token_idx = ans_token_idx[-1]\n",
    "        # create inputs as usual\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        # add padding if necessary\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.input_word_ids = input_ids\n",
    "        self.input_type_ids = token_type_ids\n",
    "        self.input_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c712a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_squad_examples(raw_data):\n",
    "#     squad_examples = []\n",
    "#     for item in raw_data[\"data\"]:\n",
    "#         for para in item[\"paragraphs\"]:\n",
    "#             context = para[\"context\"]\n",
    "#             for qa in para[\"qas\"]:\n",
    "#                 question = qa[\"question\"]\n",
    "#                 if \"answers\" in qa:\n",
    "#                     answer_text = qa[\"answers\"][0][\"text\"]\n",
    "#                     all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "#                     start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "#                     squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
    "#                 else:\n",
    "#                     squad_eg = Sample(question, context)\n",
    "#                 squad_eg.preprocess()\n",
    "#                 squad_examples.append(squad_eg)\n",
    "#     return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_word_ids\": [],\n",
    "        \"input_type_ids\": [],\n",
    "        \"input_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "#         if item.skip == False:\n",
    "        if item == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    x = [dataset_dict[\"input_word_ids\"],\n",
    "         dataset_dict[\"input_mask\"],\n",
    "         dataset_dict[\"input_type_ids\"]]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bad2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        # remove redundant whitespaces\n",
    "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
    "        # remove articles\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # get the offsets of the first and last tokens of predicted answers\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # for every pair of offsets\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            # take the required Sample object with the ground-truth answers in it\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # use offsets to get back the span of text corresponding to\n",
    "            # our predicted first and last tokens\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
    "            # clean the real answers\n",
    "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            # check if the predicted answer is in an array of the ground-truth answers\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dffd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486080 training points created.\n",
      "486080 evaluation points created.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_type_ids (InputLayer)     [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 input_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "start_logit (Dense)             (None, 384, 1)       768         keras_layer_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "end_logit (Dense)               (None, 384, 1)       768         keras_layer_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           start_logit[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 384)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 384)          0           flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,777\n",
      "Trainable params: 109,483,776\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expect x to be a non-empty array or dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a1d41e7249ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, callbacks=[ValidationCallback(x_eval, y_eval)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ]
    }
   ],
   "source": [
    "# train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "# eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
    "# with open(train_path) as f: raw_train_data = json.load(f)\n",
    "# with open(eval_path) as f: raw_eval_data = json.load(f)\n",
    "raw_train_data = text\n",
    "raw_eval_data = text\n",
    "max_seq_length = 384\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
    "\n",
    "train_squad_examples = raw_train_data\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = raw_eval_data\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=1)#, callbacks=[ValidationCallback(x_eval, y_eval)])\n",
    "model.save_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847e496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074f7924",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b740ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a2b36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e1d5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = np.random.randint(0,len(df))\n",
    "question = df[\"questions\"][random_num]\n",
    "text = df[\"answers\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ba0cc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 327 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, text)\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c5b8e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        101\n",
      "в          1,182\n",
      "##о       14,150\n",
      "##п       29,746\n",
      "##р       16,856\n",
      "##о       14,150\n",
      "##с       29,747\n",
      "о          1,193\n",
      "т          1,197\n",
      "##д       29,742\n",
      "##в       25,529\n",
      "д          1,184\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##н       18,947\n",
      "##о       14,150\n",
      "и          1,188\n",
      "х          1,200\n",
      "##о       14,150\n",
      "##р       16,856\n",
      "##о       14,150\n",
      "##ш       29,753\n",
      "##о       14,150\n",
      "о          1,193\n",
      "##т       22,919\n",
      "##д       29,742\n",
      "##ы       29,113\n",
      "##х       29,750\n",
      "##а       10,260\n",
      "##е       15,290\n",
      "##м       29,745\n",
      "л          1,190\n",
      "##и       10,325\n",
      "##ч       29,752\n",
      "##н       18,947\n",
      "##о       14,150\n",
      "в          1,182\n",
      "##а       10,260\n",
      "##м       29,745\n",
      "з          1,187\n",
      "##д       29,742\n",
      "##е       15,290\n",
      "##с       29,747\n",
      "##ь       23,742\n",
      "к          1,189\n",
      "##о       14,150\n",
      "##г       29,741\n",
      "##о       14,150\n",
      "с          1,196\n",
      "##ов      19,259\n",
      "##е       15,290\n",
      "##т       22,919\n",
      "##ов      19,259\n",
      "##а       10,260\n",
      "##л       29,436\n",
      "##и       10,325\n",
      "з          1,187\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##е       15,290\n",
      "##с       29,747\n",
      "##т       22,919\n",
      "##и       10,325\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "х          1,200\n",
      "##о       14,150\n",
      "##м       29,745\n",
      "##я       17,432\n",
      "##ч       29,752\n",
      "##ка      28,598\n",
      ".          1,012\n",
      "м          1,191\n",
      "##у       29,748\n",
      "##ж       29,743\n",
      "##и       10,325\n",
      "##ка      28,598\n",
      ",          1,010\n",
      "и          1,188\n",
      "##о       14,150\n",
      "##п       29,746\n",
      "##а       10,260\n",
      "##р       16,856\n",
      "##я       17,432\n",
      ",          1,010\n",
      "с          1,196\n",
      "##о       14,150\n",
      "##б       29,740\n",
      "##а       10,260\n",
      "##ч       29,752\n",
      "##к       23,925\n",
      "##у       29,748\n",
      "и          1,188\n",
      "50         2,753\n",
      "к          1,189\n",
      "##о       14,150\n",
      "##ш       29,753\n",
      "##е       15,290\n",
      "##к       23,925\n",
      ".          1,012\n",
      "о          1,193\n",
      "##б       29,740\n",
      "##щ       29,754\n",
      "##е       15,290\n",
      "##н       18,947\n",
      "##и       10,325\n",
      "##е       15,290\n",
      "!            999\n",
      "п          1,194\n",
      "##а       10,260\n",
      "##у       29,748\n",
      "##ч       29,752\n",
      "##ка      28,598\n",
      ".          1,012\n",
      "д          1,184\n",
      "##а       10,260\n",
      "п          1,194\n",
      "##о       14,150\n",
      "##л       29,436\n",
      "м          1,191\n",
      "##н       18,947\n",
      "##е       15,290\n",
      "б          1,181\n",
      "##ы       29,113\n",
      "п          1,194\n",
      "##а       10,260\n",
      "##м       29,745\n",
      "##ы       29,113\n",
      "##т       22,919\n",
      "##ь       23,742\n",
      "!            999\n",
      "а          1,180\n",
      "т          1,197\n",
      "##а       10,260\n",
      "##г       29,741\n",
      "т          1,197\n",
      "##о       14,150\n",
      "н          1,192\n",
      "##и       10,325\n",
      "ч          1,202\n",
      "##о       14,150\n",
      ".          1,012\n",
      "т          1,197\n",
      "##и       10,325\n",
      "##п       29,746\n",
      "##а       10,260\n",
      "н          1,192\n",
      "##и       10,325\n",
      "к          1,189\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##о       14,150\n",
      "!            999\n",
      "я          1,210\n",
      "т          1,197\n",
      "##у       29,748\n",
      "##т       22,919\n",
      "в          1,182\n",
      "##о       14,150\n",
      "##о       14,150\n",
      "##б       29,740\n",
      "##щ       29,754\n",
      "##е       15,290\n",
      "ч          1,202\n",
      "##т       22,919\n",
      "##о       14,150\n",
      "б          1,181\n",
      "##ы       29,113\n",
      "п          1,194\n",
      "##о       14,150\n",
      "##о       14,150\n",
      "##б       29,740\n",
      "##щ       29,754\n",
      "##а       10,260\n",
      "##т       22,919\n",
      "##ь       23,742\n",
      "##с       29,747\n",
      "##я       17,432\n",
      ".          1,012\n",
      "а          1,180\n",
      "м          1,191\n",
      "##н       18,947\n",
      "##е       15,290\n",
      "с          1,196\n",
      "##ов      19,259\n",
      "##е       15,290\n",
      "##т       22,919\n",
      "##ов      19,259\n",
      "##а       10,260\n",
      "##л       29,436\n",
      "##и       10,325\n",
      "с          1,196\n",
      "##и       10,325\n",
      "##с       29,747\n",
      "##и       10,325\n",
      "з          1,187\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##е       15,290\n",
      "##с       29,747\n",
      "##т       22,919\n",
      "##и       10,325\n",
      ".          1,012\n",
      "н          1,192\n",
      "##у       29,748\n",
      ",          1,010\n",
      "с          1,196\n",
      "##л       29,436\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##а       10,260\n",
      "б          1,181\n",
      "##о       14,150\n",
      "##г       29,741\n",
      "##у       29,748\n",
      ",          1,010\n",
      "м          1,191\n",
      "##у       29,748\n",
      "##ж       29,743\n",
      "##и       10,325\n",
      "##ка      28,598\n",
      "з          1,187\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##е       15,290\n",
      "##с       29,747\n",
      "##т       22,919\n",
      "##и       10,325\n",
      "е          1,185\n",
      "##щ       29,754\n",
      "##е       15,290\n",
      "н          1,192\n",
      "##е       15,290\n",
      "с          1,196\n",
      "##ов      19,259\n",
      "##е       15,290\n",
      "##т       22,919\n",
      "##ов      19,259\n",
      "##а       10,260\n",
      "##л       29,436\n",
      "##и       10,325\n",
      "а          1,180\n",
      "в          1,182\n",
      "##о       14,150\n",
      "##т       22,919\n",
      "с          1,196\n",
      "##в       25,529\n",
      "##а       10,260\n",
      "##т       22,919\n",
      "##а       10,260\n",
      "##т       22,919\n",
      "##ь       23,742\n",
      "к          1,189\n",
      "к          1,189\n",
      "##о       14,150\n",
      "##м       29,745\n",
      "##у       29,748\n",
      "т          1,197\n",
      "##о       14,150\n",
      "##л       29,436\n",
      "##ь       23,742\n",
      "##к       23,925\n",
      "##о       14,150\n",
      "н          1,192\n",
      "##е       15,290\n",
      "с          1,196\n",
      "##в       25,529\n",
      "##а       10,260\n",
      "##т       22,919\n",
      "##а       10,260\n",
      "##л       29,436\n",
      "##и       10,325\n",
      ".          1,012\n",
      "м          1,191\n",
      "##н       18,947\n",
      "##е       15,290\n",
      "т          1,197\n",
      "##у       29,748\n",
      "##т       22,919\n",
      "с          1,196\n",
      "##ов      19,259\n",
      "##е       15,290\n",
      "##т       22,919\n",
      "##ов      19,259\n",
      "##а       10,260\n",
      "##л       29,436\n",
      "##и       10,325\n",
      "з          1,187\n",
      "##а       10,260\n",
      "##в       25,529\n",
      "##е       15,290\n",
      "##с       29,747\n",
      "##т       22,919\n",
      "##и       10,325\n",
      "л          1,190\n",
      "##ю       29,757\n",
      "##б       29,740\n",
      "##ов      19,259\n",
      "##н       18,947\n",
      "##и       10,325\n",
      "##ка      28,598\n",
      ",          1,010\n",
      "м          1,191\n",
      "##у       29,748\n",
      "##ж       29,743\n",
      "##а       10,260\n",
      "и          1,188\n",
      "м          1,191\n",
      "##н       18,947\n",
      "##о       14,150\n",
      "##г       29,741\n",
      "##о       14,150\n",
      "к          1,189\n",
      "##о       14,150\n",
      "##ш       29,753\n",
      "##е       15,290\n",
      "##к       23,925\n",
      "п          1,194\n",
      "##р       16,856\n",
      "##и       10,325\n",
      "##в       25,529\n",
      "##е       15,290\n",
      "##т       22,919\n",
      "##и       10,325\n",
      "##к       23,925\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20c19ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index:  64\n",
      "Number of tokens in segment A:  65\n",
      "Number of tokens in segment B:  262\n"
     ]
    }
   ],
   "source": [
    "#first occurence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index: \", sep_idx)\n",
    "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx+1\n",
    "print(\"Number of tokens in segment A: \", num_seg_a)\n",
    "#number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(\"Number of tokens in segment B: \", num_seg_b)\n",
    "#creating the segment ids\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "#making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa64f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e978da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "Вопрос о тдв давно и хорошо отдыхаем лично вам здесь кого советовали завести?\n",
      "\n",
      "Answer:\n",
      "[cls].\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eab3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deeppavlov_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
