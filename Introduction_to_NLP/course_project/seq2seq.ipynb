{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dried-louisville",
   "metadata": {
    "id": "dried-louisville"
   },
   "source": [
    "### Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ortbQg9AP2m4",
   "metadata": {
    "id": "ortbQg9AP2m4"
   },
   "outputs": [],
   "source": [
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-court",
   "metadata": {
    "id": "unsigned-court"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-guatemala",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1622017926369,
     "user": {
      "displayName": "Alexey Tankov",
      "photoUrl": "",
      "userId": "01729535101672247421"
     },
     "user_tz": -180
    },
    "id": "phantom-guatemala",
    "outputId": "1c792b0c-764a-4c4f-f459-e47ac195ee4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:72.5% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:72.5% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cordless-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a805da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "pd.set_option('precision', 3)\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3185cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#     raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985f75f",
   "metadata": {},
   "source": [
    "### Paths to directories and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAFT_DATASET_PATH = 'E:/kaggle/mailru/Otvety.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec970111",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "destroyed-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = io.open(DRAFT_DATASET_PATH, encoding='UTF-8').read()#.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-resistance",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11da6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(w):\n",
    "    w = re.sub(':\\)', '', w)\n",
    "    w = re.sub('[)\"]', '', w)\n",
    "    w = re.sub('<[^>]+>', ' ', w)\n",
    "    \n",
    "    w = re.sub('\\s*\\?\\s*\\.', '?', w)\n",
    "    w = re.sub('\\s*\\!\\s*\\.', '!', w)\n",
    "    w = re.sub('\\s*\\.', '.', w)\n",
    "    w = re.sub('\\.+', '.', w)\n",
    "    \n",
    "    #     w = w.lower().strip()\n",
    "    w = re.sub('---', 'QUESTION', w)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c5686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072593541\n"
     ]
    }
   ],
   "source": [
    "# print(type(lines))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4589fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION\n",
      "вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести? \n",
      "хомячка. \n",
      "мужика, йопаря, собачку и 50 кошек. \n",
      "Общение! \n",
      "паучка. \n",
      "Да пол мне бы памыть! А таг то ни чо. Типа ни каво! \n",
      "я тут вообще что бы пообщаться. \n",
      "А мне советовали сиси завести. \n",
      "Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали. \n",
      "мне тут советовали завести любовника, мужа и много кошек  приветик. \n",
      "QUESTION\n",
      "Как парни относятся к цветным линзам? Если у девушки то зеленые глаза, то голубые. \n",
      "меня вобще прикалывает эта тема. \n",
      "когда этобыло редкость - было забавно, а когда все знают, что эта фальшивка, то уже не прикольно, как силиконовые сиськи или как налепленные синтетические волосы. \n",
      "QUESTION\n",
      "Что делать, сегодня нашёл 2 миллиона рублей? \n",
      "Если это счастье  действительно на вас свалилось, лучше пойти в милицию и заявить о находке. Такие деньги просто так не терют, а что самое интересное их неприменно будут искать и поверьте мне найдут, видел подобное в жизни\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 7777777\n",
    "text = preprocess_text(str(lines[:NUM_EXAMPLES]))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "little-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_sentence(sent):\n",
    "    sent = text.split('\\nQUESTION\\n')[1:]\n",
    "    \n",
    "    question = []\n",
    "    answer = []\n",
    "\n",
    "    for se in sent:\n",
    "        se = se.split('\\n')\n",
    "        question.append(se[0].strip())\n",
    "#         question.append(se[0])\n",
    "#         answer.append(' '.join([f'<start_answers> {s}<stop_answers>' for s in se[1:]]))\n",
    "#         answer.append(' '.join([f'<start> {s}<stop>' for s in se[1:]]))\n",
    "        answer.append(' '.join([f' {s}' for s in se[1:]]))\n",
    "    \n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handed-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести?',\n",
       " ' хомячка.   мужика, йопаря, собачку и 50 кошек.   Общение!   паучка.   Да пол мне бы памыть! А таг то ни чо. Типа ни каво!   я тут вообще что бы пообщаться.   А мне советовали сиси завести.   Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали.   мне тут советовали завести любовника, мужа и много кошек  приветик. ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, answer = split_by_sentence(text)\n",
    "question[0], answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c15ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'questions':question, 'answers':answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254e329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "color-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in df['questions']:\n",
    "#     re.sub('[^a-zA-Zа-яА-Я,.!?]+', ' ', w)\n",
    "\n",
    "# for w in df['answers']:\n",
    "#     re.sub('[^a-zA-Zа-яА-Я,.!?]+', ' ', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11eff6",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41260e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_subwords(corpus_generator, file_name=None):\n",
    "    start = time.time()\n",
    "    tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "        corpus_generator = corpus_generator, \n",
    "        target_vocab_size = 2**13)\n",
    "    if file_name:\n",
    "        tokenizer.save_to_file(file_name)\n",
    "        \n",
    "    print(f'in {round(time.time() - start)} sec.')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c392441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_en loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file('tokenizer_en')\n",
    "    print('tokenizer_en loaded')\n",
    "except UnicodeDecodeError:\n",
    "    tokenizer_en = split_by_subwords(\n",
    "        corpus_generator = (sent for sent in question + answer),\n",
    "        file_name='tokenizer_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10efa83c",
   "metadata": {},
   "source": [
    "### Prep datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a352e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8603, 40) (8603, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 630,   29,  304, ...,    0,    0,    0],\n",
       "        [ 113, 5238, 8433, ...,    0,    0,    0],\n",
       "        [ 180,  507,    1, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [6911, 8433, 3499, ...,    0,    0,    0],\n",
       "        [8103,   17,    1, ...,    0,    0,    0],\n",
       "        [  91,   10,  229, ...,    0,    0,    0]], dtype=int64),\n",
       " array([[   1, 6563, 2918, ..., 3234,  669,    2],\n",
       "        [ 145, 5635,  718, ...,    9, 2063,    2],\n",
       "        [   1,   28, 1322, ..., 1481, 1290,    2],\n",
       "        ...,\n",
       "        [8433, 5998,    2, ...,    0,    0,    0],\n",
       "        [   6, 1113, 1243, ..., 6739,  996,    2],\n",
       "        [1133,   51, 1113, ...,    5, 1344,  114]], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 40\n",
    "\n",
    "token_inp = [tokenizer_en.encode(q) for q in question]\n",
    "token_tar = [tokenizer_en.encode(a) for a in answer]\n",
    "\n",
    "token_inp = tf.keras.preprocessing.sequence.pad_sequences(token_inp, maxlen=MAX_LEN, padding='post', dtype='int64')\n",
    "token_tar = tf.keras.preprocessing.sequence.pad_sequences(token_tar, maxlen=MAX_LEN, padding='post', dtype='int64')\n",
    "\n",
    "print(token_inp.shape, token_tar.shape)\n",
    "token_inp, token_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3aee75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(answer, question):\n",
    "    answer = [tokenizer_en.vocab_size] + list(answer.numpy()) + [tokenizer_en.vocab_size + 1]\n",
    "    question = [tokenizer_en.vocab_size] + list(question.numpy()) + [tokenizer_en.vocab_size + 1]\n",
    "    return answer, question\n",
    "\n",
    "def tf_encode(answer, question):\n",
    "    answer, question = tf.py_function(func=encode, inp=[answer, question], Tout=[tf.int64, tf.int64])\n",
    "    answer.set_shape([None])\n",
    "    question.set_shape([None])\n",
    "    return answer, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e8da39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64, 42), dtype=int64, numpy=\n",
      "array([[8657,   35,  123, ...,    0,    0, 8658],\n",
      "       [8657,  482,  501, ...,    0,    0, 8658],\n",
      "       [8657, 2481, 3800, ...,    0,    0, 8658],\n",
      "       ...,\n",
      "       [8657,  551,  173, ...,  342, 8447, 8658],\n",
      "       [8657,   35,  123, ...,    0,    0, 8658],\n",
      "       [8657, 8057,  280, ...,    0,    0, 8658]], dtype=int64)>, <tf.Tensor: shape=(64, 42), dtype=int64, numpy=\n",
      "array([[8657,   10,  282, ..., 1800,  449, 8658],\n",
      "       [8657,    1,    6, ...,   99,    2, 8658],\n",
      "       [8657, 1652, 7425, ..., 2682,    2, 8658],\n",
      "       ...,\n",
      "       [8657,    7,  389, ...,  353,    2, 8658],\n",
      "       [8657,  662, 8446, ...,  118,    2, 8658],\n",
      "       [8657,  576, 8433, ..., 2181,    2, 8658]], dtype=int64)>)\n",
      "in 2 sec.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((token_inp, token_tar))\n",
    "dataset = dataset.map(tf_encode)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.padded_batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(next(iter(dataset)))\n",
    "print(f'in {round(time.time() - start)} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43739624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (при просроченном долге более 10 тыс. руб., которых будут сажать в тюрьму, ТО что станет со СТРАНОЙ? Куда мы катимся\n",
      "target: мщика! что в доли с ними были? а как насчет того что безработным кредитные карты присылали? ну ладно, подождем увидем\n"
     ]
    }
   ],
   "source": [
    "for inp, tar in dataset.take(1):\n",
    "    print('input: ' + tokenizer_en.decode(inp[0][1:-2]))\n",
    "    print('target: ' + tokenizer_en.decode(tar[0][1:-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea050de",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "891fe096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = pos * (1 / np.power(10_000, (2 * (i // 2)) / np.float32(d_model)))\n",
    "    return angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b0ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0])\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d36f1c",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37a273e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    seq = seq[:, tf.newaxis, tf.newaxis, :]\n",
    "    return seq\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54794aa",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c29a1804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    dk = tf.math.sqrt(dk)\n",
    "    \n",
    "    scaled_attention_logits = matmul_qk / dk\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a14de6",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e172c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        return x\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        output, weight = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(output)\n",
    "        return output, weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f871f9",
   "metadata": {},
   "source": [
    "### Point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05645efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    seq = tf.keras.Sequential([\n",
    "        layers.Dense(dff, activation='relu'),\n",
    "        layers.Dense(d_model)\n",
    "    ])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151983c6",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7137b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c522315",
   "metadata": {},
   "source": [
    "### Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c41610fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1734db0",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0be3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687be9f",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2651b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "            \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f207801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_input, rate)\n",
    "        \n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03b91a",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caf6af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_en.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e220c",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1417e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4_000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc76be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c901579b",
   "metadata": {},
   "source": [
    "### Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34a1b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a00ed5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    \n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    acc = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    acc = tf.math.logical_and(mask, acc)\n",
    "    acc = tf.cast(acc, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    \n",
    "    acc = tf.reduce_sum(acc) / tf.reduce_sum(mask)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11867dd7",
   "metadata": {},
   "source": [
    "### Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cc46f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers, d_model, num_heads, dff,\n",
    "    input_vocab_size, target_vocab_size, \n",
    "    pe_input=input_vocab_size,\n",
    "    pe_target=target_vocab_size,\n",
    "    rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1481c8b",
   "metadata": {},
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aeae1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/train'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    transformer=transformer,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaae3c8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3b14af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64)\n",
    "# ]\n",
    "\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combine_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred, _ = transformer(\n",
    "            inp, tar_inp, True,\n",
    "            enc_padding_mask,\n",
    "            combine_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, pred)\n",
    "        \n",
    "    grad = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a54abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 6.3536 Accuracy 0.1223\n",
      "Time taken for 1 epoch: 153 secs\n",
      "Saving checkpoint for epoch 1\n",
      "\n",
      "Epoch 2 Loss 6.1867 Accuracy 0.1296\n",
      "Time taken for 1 epoch: 152 secs\n",
      "Saving checkpoint for epoch 2\n",
      "\n",
      "Epoch 3 Loss 6.0471 Accuracy 0.1358\n",
      "Time taken for 1 epoch: 153 secs\n",
      "Saving checkpoint for epoch 3\n",
      "\n",
      "Epoch 4 Loss 5.9099 Accuracy 0.1430\n",
      "Time taken for 1 epoch: 160 secs\n",
      "Saving checkpoint for epoch 4\n",
      "\n",
      "Epoch 5 Loss 5.7677 Accuracy 0.1513\n",
      "Time taken for 1 epoch: 163 secs\n",
      "Saving checkpoint for epoch 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "training = True\n",
    "\n",
    "if training:\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        for batch, (inp, tar) in enumerate(dataset):\n",
    "            train_step(inp, tar)\n",
    "        \n",
    "        print (f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "        print (f'Time taken for 1 epoch: {round(time.time() - start)} secs')\n",
    "        ckpt_manager.save()\n",
    "        print (f'Saving checkpoint for epoch {epoch + 1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b851c53",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "838e6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_en.vocab_size]\n",
    "    end_token = [tokenizer_en.vocab_size + 1]\n",
    "    \n",
    "    inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, output, False,\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask\n",
    "        )\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04b7a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(sentence):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result if i < tokenizer_en.vocab_size - 1])\n",
    "    \n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'Predicted message: {predicted_sentence}')\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4ae7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Что такое ЭБУ?\n",
      "Predicted message:  А не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' А не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не . '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator('Что такое ЭБУ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b768fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Как дела?\n",
      "Predicted message:  А не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' А не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не не . '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator('Как дела?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a691f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7cadfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb35976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fifteen-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    text_tokenizer.fit_on_texts(text)\n",
    "\n",
    "    tensor = text_tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # add , dtype='int64'\n",
    "\n",
    "    return tensor, text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "invalid-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(question, answer):\n",
    "    # creating cleaned input, output pairs\n",
    "    inp_text, targ_text = question, answer\n",
    "\n",
    "    input_tensor, inp_text_tokenizer = tokenize(inp_text)\n",
    "    target_tensor, targ_text_tokenizer = tokenize(targ_text)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_text_tokenizer, targ_text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "combined-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(627, 627)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question), len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ranking-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "input_tensor, target_tensor, inp_text, targ_text = load_dataset(question, answer)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sorted-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(text, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, text.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arctic-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 501 126 126\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "alien-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 32\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_text.word_index) + 1\n",
    "vocab_tar_size = len(targ_text.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incoming-annotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23661"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "genuine-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 395]), TensorShape([32, 1245]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eastern-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "premium-canal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (32, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "constant-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "exceptional-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fiscal-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 23661])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "parallel-proof",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "injured-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dried-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "permanent-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_text.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "            \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d804589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7730ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0db198",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator('Что такое ЭБУ?.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec40647",
   "metadata": {},
   "source": [
    "### BERT Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7b2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c79563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "\n",
    "    def preprocess(self):\n",
    "        # clean context and question\n",
    "        context = \" \".join(str(self.context).split())\n",
    "        question = \" \".join(str(self.question).split())\n",
    "        # tokenize context and question\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        # if this is validation or training sample, preprocess answer\n",
    "        if self.answer_text is not None:\n",
    "            answer = \" \".join(str(self.answer_text).split())\n",
    "            # check if end character index is in the context\n",
    "            end_char_idx = self.start_char_idx + len(answer)\n",
    "            if end_char_idx >= len(context):\n",
    "                self.skip = True\n",
    "                return\n",
    "            # mark all the character indexes in context that are also in answer     \n",
    "            is_char_in_ans = [0] * len(context)\n",
    "            for idx in range(self.start_char_idx, end_char_idx):\n",
    "                is_char_in_ans[idx] = 1\n",
    "            ans_token_idx = []\n",
    "            # find all the tokens that are in the answers\n",
    "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "                if sum(is_char_in_ans[start:end]) > 0:\n",
    "                    ans_token_idx.append(idx)\n",
    "            if len(ans_token_idx) == 0:\n",
    "                self.skip = True\n",
    "                return\n",
    "            # get start and end token indexes\n",
    "            self.start_token_idx = ans_token_idx[0]\n",
    "            self.end_token_idx = ans_token_idx[-1]\n",
    "        # create inputs as usual\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        # add padding if necessary\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.input_word_ids = input_ids\n",
    "        self.input_type_ids = token_type_ids\n",
    "        self.input_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c712a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_squad_examples(raw_data):\n",
    "#     squad_examples = []\n",
    "#     for item in raw_data['data']:\n",
    "#         for para in item['paragraphs']:\n",
    "#             context = para['context']\n",
    "#             for qa in para['qas']:\n",
    "#                 question = qa['question']\n",
    "#                 if 'answers' in qa:\n",
    "#                     answer_text = qa['answers'][0]['text']\n",
    "#                     all_answers = [_['text'] for _ in qa['answers']]\n",
    "#                     start_char_idx = qa['answers'][0]['answer_start']\n",
    "#                     squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
    "#                 else:\n",
    "#                     squad_eg = Sample(question, context)\n",
    "#                 squad_eg.preprocess()\n",
    "#                 squad_examples.append(squad_eg)\n",
    "#     return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        'input_word_ids': [],\n",
    "        'input_type_ids': [],\n",
    "        'input_mask': [],\n",
    "        'start_token_idx': [],\n",
    "        'end_token_idx': [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "#         if item.skip == False:\n",
    "        if item == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    x = [dataset_dict['input_word_ids'],\n",
    "         dataset_dict['input_mask'],\n",
    "         dataset_dict['input_type_ids']]\n",
    "    y = [dataset_dict['start_token_idx'], dataset_dict['end_token_idx']]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bad2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        # remove redundant whitespaces\n",
    "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
    "        # remove articles\n",
    "#         regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "#         text = re.sub(regex, \" \", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # get the offsets of the first and last tokens of predicted answers\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # for every pair of offsets\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            # take the required Sample object with the ground-truth answers in it\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # use offsets to get back the span of text corresponding to\n",
    "            # our predicted first and last tokens\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
    "            # clean the real answers\n",
    "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            # check if the predicted answer is in an array of the ground-truth answers\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dffd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486080 training points created.\n",
      "486080 evaluation points created.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_type_ids (InputLayer)     [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 input_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "start_logit (Dense)             (None, 384, 1)       768         keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "end_logit (Dense)               (None, 384, 1)       768         keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 384)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,777\n",
      "Trainable params: 109,483,776\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expect x to be a non-empty array or dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a1d41e7249ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, callbacks=[ValidationCallback(x_eval, y_eval)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ]
    }
   ],
   "source": [
    "# train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "# eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
    "# with open(train_path) as f: raw_train_data = json.load(f)\n",
    "# with open(eval_path) as f: raw_eval_data = json.load(f)\n",
    "raw_train_data = text\n",
    "raw_eval_data = text\n",
    "max_seq_length = 384\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
    "\n",
    "train_squad_examples = raw_train_data\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = raw_eval_data\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=1)#, callbacks=[ValidationCallback(x_eval, y_eval)])\n",
    "model.save_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847e496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eab3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deeppavlov_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
