{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dried-louisville",
   "metadata": {
    "id": "dried-louisville"
   },
   "source": [
    "### Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ortbQg9AP2m4",
   "metadata": {
    "id": "ortbQg9AP2m4"
   },
   "outputs": [],
   "source": [
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-court",
   "metadata": {
    "id": "unsigned-court"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-guatemala",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1622017926369,
     "user": {
      "displayName": "Alexey Tankov",
      "photoUrl": "",
      "userId": "01729535101672247421"
     },
     "user_tz": -180
    },
    "id": "phantom-guatemala",
    "outputId": "1c792b0c-764a-4c4f-f459-e47ac195ee4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:72.5% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:72.5% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cordless-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import string\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a805da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "pd.set_option('precision', 3)\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3185cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#     raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985f75f",
   "metadata": {},
   "source": [
    "### Paths to directories and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAFT_DATASET_PATH = 'D:/kaggle/mailru/Otvety.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec970111",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "destroyed-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = io.open(DRAFT_DATASET_PATH, encoding='UTF-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-resistance",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11da6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(w):\n",
    "    w = re.sub(':\\)', '', w)\n",
    "    w = re.sub('[)\"]', '', w)\n",
    "    w = re.sub('<[^>]+>', ' ', w)\n",
    "    \n",
    "    w = re.sub('\\s*\\?\\s*\\.', '?', w)\n",
    "    w = re.sub('\\s*\\!\\s*\\.', '!', w)\n",
    "    w = re.sub('\\s*\\.', '.', w)\n",
    "    w = re.sub('\\.+', '.', w)\n",
    "    \n",
    "    #     w = w.lower().strip()\n",
    "    w = re.sub('---', 'QUESTION', w)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c5686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072593541\n"
     ]
    }
   ],
   "source": [
    "# print(type(lines))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4589fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION\n",
      "вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести? \n",
      "хомячка. \n",
      "мужика, йопаря, собачку и 50 кошек. \n",
      "Общение! \n",
      "паучка. \n",
      "Да пол мне бы памыть! А таг то ни чо. Типа ни каво! \n",
      "я тут вообще что бы пообщаться. \n",
      "А мне советовали сиси завести. \n",
      "Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали. \n",
      "мне тут советовали завести любовника, мужа и много кошек  приветик. \n",
      "QUESTION\n",
      "Как парни относятся к цветным линзам? Если у девушки то зеленые глаза, то голубые. \n",
      "меня вобще прикалывает эта тема. \n",
      "когда этобыло редкость - было забавно, а когда все знают, что эта фальшивка, то уже не прикольно, как силиконовые сиськи или как налепленные синтетические волосы. \n",
      "QUESTION\n",
      "Что делать, сегодня нашёл 2 миллиона рублей? \n",
      "Если это счастье  действительно на вас свалилось, лучше пойти в милицию и заявить о находке. Такие деньги просто так не терют, а что самое интересное их неприменно будут искать и поверьте мне найдут, видел подобное в жизни\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 12500000\n",
    "text = preprocess_text(str(lines[:NUM_EXAMPLES]))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words('ru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "little-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_sentence(sent):\n",
    "    sent = text.split('\\nQUESTION\\n')[1:]\n",
    "    \n",
    "    question = []\n",
    "    answer = []\n",
    "\n",
    "    for se in sent:\n",
    "        se = se.split('\\n')\n",
    "        se = [i for i in se if (i not in string.punctuation)]\n",
    "        se = [morpher.parse(i.lower())[0].normal_form for i in se]\n",
    "        se = [i for i in se if i not in sw and i != '']\n",
    "\n",
    "        question.append(se[0].strip())\n",
    "        answer.append(' '.join([f' {s}' for s in se[1:]]))\n",
    "    \n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handed-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести?',\n",
       " ' хомячка.   мужика, йопаря, собачку и 50 кошек.   Общение!   паучка.   Да пол мне бы памыть! А таг то ни чо. Типа ни каво!   я тут вообще что бы пообщаться.   А мне советовали сиси завести.   Ну, слава богу, мужика завести ещё не советовали А вот сватать к кому только не сватали.   мне тут советовали завести любовника, мужа и много кошек  приветик. ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, context = split_by_sentence(text)\n",
    "question[0], context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c15ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'questions':question, 'contexts':context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254e329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "color-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in df['questions']:\n",
    "#     re.sub('[^a-zA-Zа-яА-Я,.!?]+', ' ', w)\n",
    "\n",
    "# for w in df['contexts']:\n",
    "#     re.sub('[^a-zA-Zа-яА-Я,.!?]+', ' ', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec40647",
   "metadata": {},
   "source": [
    "### BERT Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-january",
   "metadata": {},
   "source": [
    "https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html  \n",
    "https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html  \n",
    "\n",
    "https://huggingface.co/deepset/xlm-roberta-large-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7b2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c79563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "\n",
    "    def preprocess(self):\n",
    "        # clean context and question\n",
    "        context = \" \".join(str(self.context).split())\n",
    "        question = \" \".join(str(self.question).split())\n",
    "        # tokenize context and question\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        # if this is validation or training sample, preprocess answer\n",
    "        if self.answer_text is not None:\n",
    "            answer = \" \".join(str(self.answer_text).split())\n",
    "            # check if end character index is in the context\n",
    "            end_char_idx = self.start_char_idx + len(answer)\n",
    "            if end_char_idx >= len(context):\n",
    "                self.skip = True\n",
    "                return\n",
    "            # mark all the character indexes in context that are also in answer     \n",
    "            is_char_in_ans = [0] * len(context)\n",
    "            for idx in range(self.start_char_idx, end_char_idx):\n",
    "                is_char_in_ans[idx] = 1\n",
    "            ans_token_idx = []\n",
    "            # find all the tokens that are in the answers\n",
    "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "                if sum(is_char_in_ans[start:end]) > 0:\n",
    "                    ans_token_idx.append(idx)\n",
    "            if len(ans_token_idx) == 0:\n",
    "                self.skip = True\n",
    "                return\n",
    "            # get start and end token indexes\n",
    "            self.start_token_idx = ans_token_idx[0]\n",
    "            self.end_token_idx = ans_token_idx[-1]\n",
    "        # create inputs as usual\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        # add padding if necessary\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.input_word_ids = input_ids\n",
    "        self.input_type_ids = token_type_ids\n",
    "        self.input_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c712a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_squad_examples(raw_data):\n",
    "#     squad_examples = []\n",
    "#     for item in raw_data['data']:\n",
    "#         for para in item['paragraphs']:\n",
    "#             context = para['context']\n",
    "#             for qa in para['qas']:\n",
    "#                 question = qa['question']\n",
    "#                 if 'answers' in qa:\n",
    "#                     answer_text = qa['answers'][0]['text']\n",
    "#                     all_answers = [_['text'] for _ in qa['answers']]\n",
    "#                     start_char_idx = qa['answers'][0]['answer_start']\n",
    "#                     squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
    "#                 else:\n",
    "#                     squad_eg = Sample(question, context)\n",
    "#                 squad_eg.preprocess()\n",
    "#                 squad_examples.append(squad_eg)\n",
    "#     return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        'input_word_ids': [],\n",
    "        'input_type_ids': [],\n",
    "        'input_mask': [],\n",
    "        'start_token_idx': [],\n",
    "        'end_token_idx': [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "#         if item.skip == False:\n",
    "        if item == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    x = [dataset_dict['input_word_ids'],\n",
    "         dataset_dict['input_mask'],\n",
    "         dataset_dict['input_type_ids']]\n",
    "    y = [dataset_dict['start_token_idx'], dataset_dict['end_token_idx']]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bad2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        # remove redundant whitespaces\n",
    "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
    "        # remove articles\n",
    "#         regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "#         text = re.sub(regex, \" \", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # get the offsets of the first and last tokens of predicted answers\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # for every pair of offsets\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            # take the required Sample object with the ground-truth answers in it\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # use offsets to get back the span of text corresponding to\n",
    "            # our predicted first and last tokens\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
    "            # clean the real answers\n",
    "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            # check if the predicted answer is in an array of the ground-truth answers\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-accordance",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representations from Transformers (BERT).  \n",
    "https://hub.tensorflow.google.cn/tensorflow/bert_multi_cased_L-12_H-768_A-12/4   \n",
    "\n",
    "Language-agnostic BERT sentence embedding model supporting 109 languages.  \n",
    "https://tfhub.dev/google/LaBSE/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dffd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486080 training points created.\n",
      "486080 evaluation points created.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_type_ids (InputLayer)     [(None, 384)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 input_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "start_logit (Dense)             (None, 384, 1)       768         keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "end_logit (Dense)               (None, 384, 1)       768         keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 384)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,777\n",
      "Trainable params: 109,483,776\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expect x to be a non-empty array or dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a1d41e7249ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, callbacks=[ValidationCallback(x_eval, y_eval)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ]
    }
   ],
   "source": [
    "# 'bert-base-multilingual-cased'\n",
    "# train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "# eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
    "# with open(train_path) as f: raw_train_data = json.load(f)\n",
    "# with open(eval_path) as f: raw_eval_data = json.load(f)\n",
    "raw_train_data = text\n",
    "raw_eval_data = text\n",
    "max_seq_length = 384\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
    "\n",
    "train_squad_examples = raw_train_data\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = raw_eval_data\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=1)#, callbacks=[ValidationCallback(x_eval, y_eval)])\n",
    "model.save_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847e496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eab3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "deeppavlov_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
